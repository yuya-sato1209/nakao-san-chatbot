import streamlit as st
# â–¼â–¼â–¼ æœ€æ–°ã®LangChainãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆLCELï¼‰ã‚’ä½¿ç”¨ â–¼â–¼â–¼
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableBranch

# â–¼â–¼â–¼ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ç”¨ â–¼â–¼â–¼
from langchain_community.retrievers import BM25Retriever
import re # æ­£è¦è¡¨ç¾ç”¨ï¼ˆã‚¯ã‚¨ãƒªåˆ†æã«ä½¿ç”¨ï¼‰

# ãã®ä»–ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from dotenv import load_dotenv
import os
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
import pytz
import json

# --- å®šæ•°å®šç¾© ---
SPREADSHEET_ID = "1xeuewRd2GvnLDpDYFT5IJ5u19PUhBOuffTfCyWmQIzA"

# --- Streamlit UIè¨­å®š ---
st.set_page_config(page_title="ãƒŠã‚«ã‚ªã•ã‚“ã®å‡½é¤¨æ­´å²æ¢è¨ª", layout="wide")
st.title("ğŸ“ ãƒŠã‚«ã‚ªã•ã‚“ã®å‡½é¤¨æ­´å²æ¢è¨ª")

# --- APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ ---
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")

if not openai_api_key:
    st.error("OpenAI APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Streamlitã®Secretsã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

os.environ["OPENAI_API_KEY"] = openai_api_key

# --- æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆå½¢æ…‹ç´ è§£æï¼‰ ---
def get_japanese_tokenizer():
    try:
        from fugashi import Tagger
        tagger = Tagger('-Owakati')
        def tokenize(text):
            return tagger.parse(text).split()
        return tokenize
    except ImportError:
        st.warning("âš ï¸ 'fugashi' ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚BM25ã®ç²¾åº¦ãŒè½ã¡ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return lambda text: list(text)

japanese_tokenizer = get_japanese_tokenizer()

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•° ---
@st.cache_data
def load_raw_data():
    all_data = []
    try:
        with open("rag_data_cleaned.jsonl", "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    try:
                        data = json.loads(line)
                        if data.get("text") and data.get("text").strip():
                            all_data.append(data)
                    except json.JSONDecodeError:
                        pass
    except FileNotFoundError:
        return []
    return all_data

# --- ã€æ–°è¦å®Ÿè£…ã€‘RRFã‚¹ã‚³ã‚¢ãƒ»é–¾å€¤ãƒ»å‹•çš„é‡ã¿ä»˜ã‘å¯¾å¿œãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ ---
class RRFEnsembleRetriever:
    # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å®šæ•°ï¼ˆretrieversãƒªã‚¹ãƒˆã®é †åºã«å¯¾å¿œï¼‰
    BM25_IDX = 0
    FAISS_IDX = 1

    def __init__(self, retrievers, k=4, c=60, score_threshold=0.02):
        self.retrievers = retrievers
        self.k = k               # æœ€çµ‚çš„ã«æ®‹ã™ä»¶æ•°
        self.c = c               # RRFã®å®šæ•°
        self.score_threshold = score_threshold # è¶³åˆ‡ã‚Šãƒ©ã‚¤ãƒ³

    def get_dynamic_weights(self, query):
        """è³ªå•ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦é‡ã¿ã‚’å‹•çš„ã«å¤‰ãˆã‚‹"""
        # å›ºæœ‰åè©ã‚„äº‹å®Ÿã‚’å•ã†ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        keyword_pattern = r"(èª°|ã©ã“|ã„ã¤|ä½•å¹´|åå‰|å ´æ‰€|å»ºç‰©|äººç‰©|ä½•ã¦|ãªã‚“ã¦)"
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®é‡ã¿ãƒªã‚¹ãƒˆã‚’ä½œæˆï¼ˆè¦ç´ æ•°åˆ†ï¼‰
        weights = [0.5] * len(self.retrievers)

        if re.search(keyword_pattern, query):
            # å›ºæœ‰åè©ç³»è³ªå• -> BM25(ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰)é‡è¦–
            weights[self.BM25_IDX] = 0.7
            weights[self.FAISS_IDX] = 0.3
        else:
            # æŠ½è±¡çš„ãƒ»èª¬æ˜ç³»è³ªå• -> FAISS(æ„å‘³)é‡è¦–
            weights[self.BM25_IDX] = 0.3
            weights[self.FAISS_IDX] = 0.7
            
        return weights

    def invoke(self, query):
        doc_scores = {}
        doc_map = {}
        
        # ã‚¯ã‚¨ãƒªã‚’åˆ†æã—ã¦é‡ã¿ã‚’æ±ºå®š
        current_weights = self.get_dynamic_weights(query)

        # å„ãƒªãƒˆãƒªãƒ¼ãƒãƒ¼ã§æ¤œç´¢ã‚’å®Ÿè¡Œ
        for i, retriever in enumerate(self.retrievers):
            weight = current_weights[i]
            
            try:
                docs = retriever.invoke(query)
            except AttributeError:
                docs = retriever.get_relevant_documents(query)
            
            for rank, doc in enumerate(docs):
                # å†…å®¹ã‚’ã‚­ãƒ¼ã«ã—ã¦é‡è¤‡ã‚’ãƒãƒ¼ã‚¸
                key = doc.page_content
                if key not in doc_map:
                    doc_map[key] = doc
                    doc_scores[key] = 0.0
                
                # é‡ã¿ä»˜ãRRFã‚¹ã‚³ã‚¢ã®åŠ ç®—
                # weightãŒé«˜ã„ã»ã©ã€ãã®æ¤œç´¢æ©Ÿã§è¦‹ã¤ã‹ã£ãŸã“ã¨ã®ä¾¡å€¤ãŒé«˜ã¾ã‚‹
                doc_scores[key] += weight * (1.0 / (self.c + rank + 1))
        
        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆï¼ˆã‚¹ã‚³ã‚¢ãŒé«˜ã„é †ï¼‰
        sorted_items = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        
        # ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆé–¾å€¤æœªæº€ã¯åˆ‡ã‚Šæ¨ã¦ï¼‰
        filtered_docs = []
        for key, score in sorted_items:
            if score >= self.score_threshold:
                filtered_docs.append(doc_map[key])
        
        # ã€æ”¹å–„ã€‘ã‚‚ã—é–¾å€¤ã§çµæœãŒå°‘ãªã™ãã‚‹å ´åˆã¯ã€æœ€ä½2ä»¶ã¯æ•‘æ¸ˆã—ã¦æ®‹ã™
        if len(filtered_docs) < 2 and sorted_items:
            # ã‚¹ã‚³ã‚¢ä¸Šä½2ä»¶ã‚’å¼·åˆ¶çš„ã«æ¡ç”¨ï¼ˆã‚¹ãƒ©ã‚¤ã‚¹ç¯„å›²å¤–ãªã‚‰å…¨ä»¶ã«ãªã‚‹ã®ã§å®‰å…¨ï¼‰
            filtered_docs = [doc_map[k] for k, _ in sorted_items[:2]]
        
        # ä¸Šä½kä»¶ã‚’è¿”ã™
        return filtered_docs[:self.k]

    # LCEL äº’æ›
    def __call__(self, query):
        return self.invoke(query)


# --- æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ ---
@st.cache_resource
def setup_retrievers(_raw_data):
    if not _raw_data:
        return None

    # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ
    documents = []
    for data in _raw_data:
        doc = Document(
            page_content=data["text"],
            metadata={
                "source_video": data.get("source_video", "ä¸æ˜ãªã‚½ãƒ¼ã‚¹"),
                "url": data.get("url", "#")
            }
        )
        documents.append(doc)

    if not documents:
        return None

    # 2. ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = splitter.split_documents(documents)
    
    if not split_docs:
        return None

    # 3. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æ©Ÿ (FAISS)
    try:
        embedding = OpenAIEmbeddings(model="text-embedding-3-small")
        vectorstore = FAISS.from_documents(split_docs, embedding=embedding)
        # RRFã«ã‹ã‘ã‚‹å‰ãªã®ã§ã€å¤šã‚ã«å€™è£œã‚’å–ã‚‹ (k=10)
        faiss_retriever = vectorstore.as_retriever(search_kwargs={'k': 10})
    except Exception as e:
        st.error(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®æ§‹ç¯‰ã«å¤±æ•—: {e}")
        return None

    # 4. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢æ©Ÿ (BM25)
    try:
        # ã€æ”¹å–„ã€‘kã‚’åˆæœŸåŒ–æ™‚ã«æŒ‡å®šã—ã¦ç¢ºå®Ÿã«åæ˜ ã•ã›ã‚‹
        bm25_retriever = BM25Retriever.from_documents(
            split_docs,
            preprocess_func=japanese_tokenizer,
            k=10 # ã“ã¡ã‚‰ã‚‚å¤šã‚ã«å–ã‚‹
        )
    except Exception as e:
        st.warning(f"BM25æ¤œç´¢ã®æ§‹ç¯‰ã«å¤±æ•—ï¼ˆFAISSã®ã¿ä½¿ç”¨ï¼‰: {e}")
        return faiss_retriever

    # 5. é«˜æ©Ÿèƒ½ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢æ©Ÿ (RRF + å‹•çš„é‡ã¿ + ãƒ•ã‚£ãƒ«ã‚¿)
    try:
        # Retrieversã®ãƒªã‚¹ãƒˆé †åº: [0]=BM25, [1]=FAISS (ã‚¯ãƒ©ã‚¹å®šæ•°ã¨åˆã‚ã›ã‚‹)
        ensemble_retriever = RRFEnsembleRetriever(
            retrievers=[bm25_retriever, faiss_retriever], 
            k=4, # æœ€çµ‚çš„ã«æ®‹ã™ã®ã¯4ä»¶
            score_threshold=0.015 # é–¾å€¤
        )
        return ensemble_retriever
    except Exception as e:
        st.error(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®æ§‹ç¯‰ã«å¤±æ•—: {e}")
        return faiss_retriever


# ==================================================
# â–¼â–¼â–¼ LCELã«ã‚ˆã‚‹ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰ â–¼â–¼â–¼
# ==================================================

# LLMã®æº–å‚™
llm = ChatOpenAI(model_name="gpt-5.1", temperature=0.4)
raw_data = load_raw_data()
retriever = setup_retrievers(raw_data)

if not retriever:
    st.error("çŸ¥è­˜æºãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.stop()

# 1. æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
contextualize_q_system_prompt = """
ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã¨æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ãŒã‚ã‚Šã¾ã™ã€‚
ã“ã®è³ªå•ã¯éå»ã®æ–‡è„ˆã«é–¢é€£ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è€ƒæ…®ã—ã¦ã€ã“ã®è³ªå•ã‚’ã€Œå˜ä½“ã§ç†è§£ã§ãã‚‹ç‹¬ç«‹ã—ãŸè³ªå•æ–‡ã€ã«æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚
è³ªå•ã«ç­”ãˆã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ›¸ãæ›ãˆãŸè³ªå•æ–‡ã ã‘ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
ã¾ãŸã€å›ºæœ‰åè©ã®èª¤å­—ï¼ˆä¾‹ï¼šã€ŒæŸ³å·ç†Šã€â†’ã€ŒæŸ³å·ç†Šå‰ã€ï¼‰ãŒã‚ã‚Œã°è¨‚æ­£ã—ã¦ãã ã•ã„ã€‚
"""
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

query_transform_chain = RunnableBranch(
    (
        lambda x: not x.get("chat_history", []),
        (lambda x: x["input"])
    ),
    contextualize_q_prompt | llm | StrOutputParser()
)

# 2. å›ç­”ç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
qa_system_prompt = """
ã‚ãªãŸã¯ã€å‡½é¤¨ã®æ­´å²ã‚’æ¡ˆå†…ã™ã‚‹ãƒ™ãƒ†ãƒ©ãƒ³ã‚¬ã‚¤ãƒ‰ã®ã€ŒãƒŠã‚«ã‚ªã•ã‚“ã€ã§ã™ã€‚
ã‚¬ã‚¤ãƒ‰ã•ã‚“ãŒä¸€æ–¹çš„ã«å–‹ã‚Šç¶šã‘ã‚‹ã®ã§ã¯ãªãã€å¯¾è©±ã‚’ä¿ƒã™ã‚¹ã‚¿ã‚¤ãƒ«ã§ç­”ãˆã¦ãã ã•ã„ã€‚
1å›ç­”ã‚ãŸã‚Š300~500å­—ç¨‹åº¦ã§å›ç­”ã¯é•·ããªã‚Šã™ããªã„ã‚ˆã†ã«æ³¨æ„ã—ã€ç›¸æ‰‹ã®åå¿œã‚’å¾…ã¤ä½™è£•ã‚’æŒã£ã¦ãã ã•ã„ã€‚
AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã¨ã—ã¦ã®ç¡¬ã„å£èª¿ã¯æ¨ã¦ã¦ã€ä»¥ä¸‹ã®ã€è©±ã—æ–¹ã®ç‰¹å¾´ã€‘ã‚’å‚è€ƒã«ã€äººé–“å‘³ã‚ãµã‚Œã‚‹èªã‚Šå£ã§ç­”ãˆã¦ãã ã•ã„ã€‚

ã€è©±ã—æ–¹ã®ç‰¹å¾´ã€‘
1. æ§‹æˆã®ãƒ‘ã‚¿ãƒ¼ãƒ³
1.1 å°å…¥ï¼ˆæ™‚ä»£ãƒ»èƒŒæ™¯ï¼‰ã€Œã€‡ã€‡ã«ã¤ã„ã¦ã§ã™ã­ã€‚å®Ÿã¯é¢ç™½ã„è©±ãŒã‚ã‚Šã¾ã—ã¦ï½ã€ãªã©ã®è³ªå•è€…ãŒèˆˆå‘³ã‚’æŒã¡ãã†å‡ºã ã—ã§å§‹ã¾ã‚Šã€ã€Œæ˜æ²»ã€‡å¹´ã€ã€Œå®‰æ”¿ã€‡å¹´ã€ã¨ã„ã£ãŸå…·ä½“çš„ãªå…ƒå·ã¨è¥¿æš¦ã€ãã®å½“æ™‚ã®æ™‚ä»£èƒŒæ™¯ï¼ˆé–‹æ¸¯ã€æˆ¦äº‰ã€å¤§ç«ãªã©ï¼‰ã‚’ç°¡æ½”ã«èª¬æ˜ã™ã‚‹ã€‚
1.2 å±•é–‹ï¼ˆäººç‰©ãƒ»ãƒ‰ãƒ©ãƒï¼‰ ç‰¹å®šã®äººç‰©ï¼ˆé«˜ç”°å±‹å˜‰å…µè¡›ã€ãƒšãƒªãƒ¼ã€åœ°å…ƒã®åå£«ãªã©ï¼‰ã«ç„¦ç‚¹ã‚’å½“ã¦ã€ãã®äººç‰©ãŒã©ã®ã‚ˆã†ãªè‹¦åŠ´ã‚’ã—ãŸã‹ã€ã©ã®ã‚ˆã†ãªåŠŸç¸¾ã‚’æ®‹ã—ãŸã‹ã¨ã„ã†ã€Œç‰©èªã€ã‚’èªã‚‹ã€‚
1.3 çµã³ï¼ˆç¾åœ¨ã¨ã®ã¤ãªãŒã‚Š)ã€Œç¾åœ¨ã¯â—‹â—‹ã¨ãªã£ã¦ã„ã‚‹ã€ã€Œç¢‘ãŒå»ºã£ã¦ã„ã‚‹ã€ã€Œé¢å½±ã‚’æ®‹ã—ã¦ã„ã‚‹ã€ãªã©ã€ç¾ä»£ã®é¢¨æ™¯ã‚„ç—•è·¡ã«ç€åœ°ã•ã›ã¦ç· ã‚ããã‚‹ã€‚
2. æ–‡ä½“ãƒ»ãƒˆãƒ¼ãƒ³
2.1ã€Œã§ã™ãƒ»ã¾ã™ã€èª¿ï¼ˆæ•¬ä½“ï¼‰: åŸºæœ¬çš„ã«ä¸å¯§ãªèªã‚Šå£ã§ã€ã‚¬ã‚¤ãƒ‰ãŒå®¢ã«èª¬æ˜ã—ã¦ã„ã‚‹ã‚ˆã†ãªãƒˆãƒ¼ãƒ³ã€‚å…·ä½“çš„ã«ã¯è¦ªã—ã¿ã‚„ã™ã„å£èªä½“ã€Œï½ãªã‚“ã§ã™ã€ã€Œï½ã§ã—ã¦ã­ã€ã€Œï½ã¨è¨€ã‚ã‚Œã¦ãŠã‚Šã¾ã™ã€ã€Œå®Ÿã¯ï½ãªã‚“ã§ã™ã‚ˆã€ã‚’å¤šç”¨ã™ã‚‹ã€‚
2.2 è¬›è«‡èª¿ãƒ»ç‰©èªèª¿: çŸ¥è­˜ã‚’æŠ«éœ²ã™ã‚‹ã ã‘ã§ãªãã€ã€Œã“ã“ã ã‘ã®è©±ï½ã€ã€Œï½ã¨ã„ã†é‹å‘½ã®çš®è‚‰ã¨ã—ã‹è¨€ã„æ§˜ãŒãªã„ã€ã€Œï½ã¨å£ã€…ã«ã•ã•ã‚„ãã‚ã£ãŸã€ãªã©ã€æ„Ÿæƒ…ã«è¨´ãˆã‹ã‘ã‚‹ã‚ˆã†ãªã€å°‘ã—åŠ‡çš„ãªè¡¨ç¾ãŒå«ã¾ã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚
2.3 åœ°å…ƒæ„›ãƒ»èª‡ã‚Š: ã€ŒåŒ—æµ·é“æœ€åˆã®ï½ã€ã€Œæ—¥æœ¬å±ˆæŒ‡ã®ï½ã€ã€Œå‡½é¤¨ã®èª‡ã‚Šã€ã¨ã„ã£ãŸã€åœ°åŸŸä¸€ç•ªã‚„æ—¥æœ¬åˆã‚’å¼·èª¿ã™ã‚‹ãƒ•ãƒ¬ãƒ¼ã‚ºãŒå¤šãè¦‹ã‚‰ã‚Œã‚‹ã€‚
3. ç‰¹å¾´çš„ãªèªå½™ãƒ»è¡¨ç¾
3.1 å²ç”¨èª: ã€Œé–‹æ‹“ä½¿ã€ã€Œç®±é¤¨å¥‰è¡Œã€ã€Œå¤§ç«ã€ã€Œå±…ç•™åœ°ã€ãªã©ã€å‡½é¤¨ç‰¹æœ‰ã®æ­´å²ç”¨èªãŒé »å‡ºã™ã‚‹ã€‚
3.2 å¼•ç”¨ãƒ»å‡ºå…¸: ã€Œï½ã¨è¨€ã‚ã‚Œã¦ã„ã‚‹ã€ã€Œï½ã¨ã„ã†èª¬ã‚‚ã‚ã‚‹ã€ã¨ã„ã£ãŸä¼èå½¢å¼ãŒæ–‡æœ«ã‚„æ–‡ä¸­ã«è¦‹ã‚‰ã‚Œã‚‹ã€‚
4. å…·ä½“ä¾‹
4.1 æ›¸ãå‡ºã—: ã€Œæ˜æ²»â—‹å¹´ã€ï½ãŒè¨­ç«‹ã•ã‚Œã¾ã—ãŸã€‚ã€ã€Œï½ã¯ã€ã€‡ã€‡ã«ç”±æ¥ã—ã¾ã™ã€‚ã€
4.2 æ„Ÿæƒ…ç§»å…¥: ã€Œå¤±æ„ã®ã†ã¡ã«ï½ã€ã€Œæ³¢ä¹±ä¸‡ä¸ˆã®äººç”Ÿã§ã‚ã£ãŸã€ã€Œå¸‚æ°‘ã«æƒœã—ã¾ã‚Œã¤ã¤ï½ã€
4.3 ç¾çŠ¶èª¬æ˜: ã€Œç¾åœ¨ã¯ï½ã¨ã—ã¦ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ã€ã€Œè¨˜å¿µç¢‘ãŒã²ã£ãã‚Šã¨å»ºã£ã¦ã„ã‚‹ã€‚ã€
4.4 å ´æ‰€èª¬æ˜: æ–‡ç« ã®æœ€åˆã§åˆã‚ã¦ã§ã‚‹å ´æ‰€ã®èª¬æ˜ã‚’ã™ã‚‹ã¨ãã«ã€Œãã“ã€ã‚„ã€Œã“ã®ã‚ãŸã‚Šã€ãªã©ã®æŠ½è±¡çš„ãªã‚‚ã®ã§ãªãã€å…·ä½“çš„ãªå ´æ‰€ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚
5.ã€€è©±ã—æ–¹ã®ãƒ«ãƒ¼ãƒ«
5.1 è¦‹å‡ºã—ãƒ»ç®‡æ¡æ›¸ãã®ç¦æ­¢:ä¼šè©±ã«ã€Œè¦‹å‡ºã—ã€ã‚„ã€Œç®‡æ¡æ›¸ãã€ã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚ã™ã¹ã¦æ®µè½ã¨æ¥ç¶šè©ï¼ˆã€Œã¨ã“ã‚ã§ã€ã€Œæ¬¡ã«ã€ãªã©ï¼‰ã§ç¹‹ã„ã§è©±ã—ã¦ãã ã•ã„ã€‚
5.2 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ã‚¿ãƒªãƒ¼èª¿ã®ç¦æ­¢:ã€Œæ™‚ã¯æ˜æ²»ã€‡å¹´â€•â€•ã€ã®ã‚ˆã†ãªå°èª¬çš„ãªæ›¸ãå‡ºã—ã¯é¿ã‘ã€ã€Œæ˜æ²»ã€‡å¹´ã®è©±ã«ãªã‚Šã¾ã™ãŒã­ã€ã€ã¨è‡ªç„¶ã«åˆ‡ã‚Šå‡ºã—ã¦ãã ã•ã„ã€‚
5.3 ã€ŒçŸ¥ã‚‰ãªã„ã€æ™‚ã®äººé–“å‘³:æƒ…å ±ãŒãªã„å ´åˆã¯ã€Œæ‰‹å…ƒã®è³‡æ–™ã«ãªã„ã€ã¨äº‹å‹™çš„ã«æ–­ã‚‹ã®ã§ã¯ãªãã€ã€ŒãŠã‚„ã€ãã®ã‚ãŸã‚Šã®è©³ã—ã„ã“ã¨ã¯ã€ã‚ã„ã«ãç§ã®è¨˜æ†¶ï¼ˆè³‡æ–™ï¼‰ã«ã¯æ®‹ã£ã¦ã„ãªã„ã‚ˆã†ã§ã—ã¦â€¦ ç”³ã—è¨³ãªã„ã€ã¨ã‚¬ã‚¤ãƒ‰ã‚‰ã—ãæ¿ã—ã¦ãã ã•ã„ã€‚
6. å¯¾è©±ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ï¼ˆæœ€é‡è¦ï¼‰
6.1 ä¸€åº¦ã«ã™ã¹ã¦ã‚’èªã‚‰ãšã€ä¸€ã¤ã®è©±é¡ŒãŒçµ‚ã‚ã£ãŸã‚‰ã€Œã€œã«ã¤ã„ã¦ã¯ã”å­˜çŸ¥ã§ã™ã‹ï¼Ÿã€ã‚„ã€Œå®Ÿã¯ã“ã‚“ãªè£è©±ã‚‚ã‚ã‚‹ã‚“ã§ã™ãŒã€èãã¾ã™ï¼Ÿã€ã¨å•ã„ã‹ã‘ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ä¼šè©±ã®ã‚­ãƒ£ãƒƒãƒãƒœãƒ¼ãƒ«ã‚’é‡è¦–ã—ã¦ãã ã•ã„ã€‚
6.2 è¦ç‚¹ã‚’çµã£ã¦ã€1å›ç­”ã‚ãŸã‚Š200~300å­—ç¨‹åº¦ã§å›ç­”ã¯é•·ããªã‚Šã™ããªã„ã‚ˆã†ã«æ³¨æ„ã—ã€ç›¸æ‰‹ã®åå¿œã‚’å¾…ã¤ä½™è£•ã‚’æŒã£ã¦ãã ã•ã„ã€‚
6.3 ä¸€æ–¹çš„ãªè¬›ç¾©ã§ã¯ãªãã€ä¸€ç·’ã«è¡—ã‚’æ­©ã„ã¦ã„ã‚‹ã‚ˆã†ãªé›°å›²æ°—ã‚’ä½œã£ã¦ãã ã•ã„ã€‚


ã€å³æ ¼ãªãƒ«ãƒ¼ãƒ«ã€‘
1. å‚è€ƒæƒ…å ±ã®ã¿ã‚’ä½¿ç”¨: å›ç­”ã¯ã€å¿…ãšä»¥ä¸‹ã®ã€Œå‚è€ƒæƒ…å ±ã€ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’æ ¹æ‹ ã«ä½œæˆã—ã¦ãã ã•ã„ã€‚
2. å¤–éƒ¨çŸ¥è­˜ã®ç¦æ­¢: ã‚ãªãŸãŒAIã¨ã—ã¦æŒã£ã¦ã„ã‚‹ä¸€èˆ¬çš„ãªçŸ¥è­˜ã‚„ã€å‚è€ƒæƒ…å ±ã«ãªã„äº‹æŸ„ã¯ã€çµ¶å¯¾ã«å›ç­”ã«å«ã‚ãªã„ã§ãã ã•ã„ã€‚
3. ä¸æ˜ãªå ´åˆã®å¯¾å¿œ: ã‚‚ã—ã€Œå‚è€ƒæƒ…å ±ã€ã®ä¸­ã«è³ªå•ã®ç­”ãˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ç„¡ç†ã«å‰µä½œã›ãšæ­£ç›´ã«ç­”ãˆã¦ãã ã•ã„ã€‚



ã€å‚è€ƒæƒ…å ±ã€‘
{context}
"""
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å½¢é–¢æ•°ï¼ˆç©ºãƒªã‚¹ãƒˆãªã‚‰ç©ºæ–‡å­—ã‚’è¿”ã™ï¼‰
def format_docs(docs):
    return "\n\n".join([d.page_content for d in docs]) if docs else ""

# 3. çµ±åˆãƒã‚§ãƒ¼ãƒ³
rag_chain = (
    RunnablePassthrough.assign(
        context_docs=query_transform_chain | retriever
    )
    .assign(
        context=lambda x: format_docs(x["context_docs"])
    )
    .assign(
        answer=qa_prompt | llm | StrOutputParser()
    )
)


# --- Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æº ---
@st.cache_resource
def connect_to_gsheet():
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = Credentials.from_service_account_info(creds_dict)
        scoped_creds = creds.with_scopes([
            "https://www.googleapis.com/auth/spreadsheets"
        ])
        client = gspread.authorize(scoped_creds)
        spreadsheet = client.open_by_key(SPREADSHEET_ID)
        worksheet = spreadsheet.worksheet("log")
        return worksheet
    except Exception as e:
        return None

def append_log_to_gsheet(worksheet, username, query, response):
    if worksheet is not None:
        try:
            jst = pytz.timezone('Asia/Tokyo')
            timestamp = datetime.now(jst).strftime('%Y-%m-%d %H:%M:%S')
            worksheet.append_row([timestamp, username, query, response])
        except Exception:
            pass

worksheet = connect_to_gsheet()

# --- ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ ---
if "username" not in st.session_state:
    st.session_state.username = ""

if st.session_state.username == "":
    st.session_state.username = st.text_input("ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ã‚’å…¥åŠ›ã—ã¦ã€Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„", key="username_input")
    if st.session_state.username:
        st.rerun()
else:
    st.write(f"ã“ã‚“ã«ã¡ã¯ã€{st.session_state.username}ã•ã‚“ï¼")
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # éå»ãƒ­ã‚°ã®è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if message["role"] == "assistant":
                # å‚ç…§å…ƒã®è¡¨ç¤º
                if "source_documents" in message:
                    with st.expander("ğŸ” å›ç­”ã®æ ¹æ‹ ã¨ãªã£ãŸãƒ†ã‚­ã‚¹ãƒˆ"):
                        seen_urls = set()
                        for doc in message["source_documents"]:
                            # è¾æ›¸å½¢å¼ã‹Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã§åˆ†å²
                            if isinstance(doc, dict):
                                meta = doc.get("metadata", {})
                                content = doc.get("page_content", "")
                            else:
                                meta = doc.metadata
                                content = doc.page_content

                            video_url = meta.get("url", "#")
                            if video_url in seen_urls:
                                continue
                            seen_urls.add(video_url)
                            
                            video_title = meta.get("source_video", "ä¸æ˜ãªã‚½ãƒ¼ã‚¹")
                            st.write(f"**å‚ç…§å…ƒ:** [{video_title}]({video_url})")
                            st.write(f"> {content}")

    if query := st.chat_input("ğŸ’¬ å‡½é¤¨ã®è¡—æ­©ãã«åŸºã¥ã„ã¦è³ªå•ã—ã¦ã¿ã¦ãã ã•ã„"):
        st.session_state.messages.append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.markdown(query)

        with st.chat_message("assistant"):
            with st.spinner("è€ƒãˆä¸­..."):
                
                # ä¼šè©±å±¥æ­´ã‚’LangChainå½¢å¼ï¼ˆHumanMessage, AIMessageï¼‰ã«å¤‰æ›
                # â€»å¤ã„ã‚³ãƒ¼ãƒ‰ï¼ˆã‚¿ãƒ—ãƒ«å½¢å¼ï¼‰ãŒæ··ã–ã‚‰ãªã„ã‚ˆã†ã«æ³¨æ„
                chat_history_objs = []
                for msg in st.session_state.messages[:-1]:
                    if msg["role"] == "user":
                        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•
                        chat_history_objs.append(HumanMessage(content=msg["content"]))
                    elif msg["role"] == "assistant":
                        # AIã®å›ç­”
                        chat_history_objs.append(AIMessage(content=msg["content"]))

                # â–¼â–¼â–¼ ãƒã‚§ãƒ¼ãƒ³ã®å®Ÿè¡Œ â–¼â–¼â–¼
                result = rag_chain.invoke({
                    "input": query,
                    "chat_history": chat_history_objs
                })
                
                response = result["answer"]
                source_docs = result["context_docs"]

                st.markdown(response)
                
                append_log_to_gsheet(worksheet, st.session_state.username, query, response)
                
                with st.expander("ğŸ” å›ç­”ã®æ ¹æ‹ ã¨ãªã£ãŸãƒ†ã‚­ã‚¹ãƒˆ"):
                    seen_urls = set()
                    for doc in source_docs:
                        video_url = doc.metadata.get("url", "#")
                        if video_url in seen_urls:
                            continue
                        seen_urls.add(video_url)
                        
                        video_title = doc.metadata.get("source_video", "ä¸æ˜ãªã‚½ãƒ¼ã‚¹")
                        st.write(doc.page_content)
                        st.write(f"**å‚ç…§å…ƒ:** [{video_title}]({video_url})")

                # å±¥æ­´ä¿å­˜
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": response,
                    "source_documents": source_docs
                })