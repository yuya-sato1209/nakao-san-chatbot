
import streamlit as st
from langchain_community.chat_models import ChatOpenAI
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from dotenv import load_dotenv
import os

# --- Streamlit UI ---
st.set_page_config(page_title="ä¸­å°¾ã•ã‚“ãªã‚Šãã‚ŠChatBot", layout="wide")
st.title("ğŸ“ ä¸­å°¾ã•ã‚“ãªã‚Šãã‚ŠChatBot")

load_dotenv()  # .env ã‚’èª­ã¿è¾¼ã‚€
openai_api_key = os.getenv("OPENAI_API_KEY")

if not openai_api_key:
    raise ValueError("OpenAI APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

os.environ["OPENAI_API_KEY"] = openai_api_key

# --- RAGç”¨ãƒ™ã‚¯ãƒˆãƒ«DBã®æ§‹ç¯‰ (ã“ã®éƒ¨åˆ†ã¯å¤‰æ›´ãªã—) ---
@st.cache_resource
def load_vectorstore():
    loader = TextLoader("rag_trainning.txt", encoding="utf-8")
    documents = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    docs = splitter.split_documents(documents)
    embedding = OpenAIEmbeddings()
    vectordb = FAISS.from_documents(docs, embedding=embedding)
    return vectordb

vectordb = load_vectorstore()
retriever = vectordb.as_retriever()

# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ (ã“ã®éƒ¨åˆ†ã¯å¤‰æ›´ãªã—) ---
# â–¼â–¼â–¼ ã“ã® template ã®å†…å®¹ã‚’å·®ã—æ›¿ãˆã¦ãã ã•ã„ â–¼â–¼â–¼
template = """
ã‚ãªãŸã¯Aã•ã‚“æœ¬äººã¨ã—ã¦ã€å‡½é¤¨ã®è¡—æ­©ãã«å‚åŠ ã—ãŸäººãŸã¡ã‹ã‚‰ã®è³ªå•ã«ç­”ãˆã¾ã™ã€‚
å£èª¿ãƒ»èªå°¾ãƒ»è©±ã—æ–¹ã®ç™–ãƒ»æ€è€ƒã®ç‰¹å¾´ãªã©ã¯ã€ä»¥ä¸‹ã®è¬›é¤¨ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å¿ å®Ÿã«å­¦ã³ã€å†ç¾ã—ã¦ãã ã•ã„ã€‚

ä»¥ä¸‹ã«ã€å›ç­”ã®å‚è€ƒã«ãªã‚‹Aã•ã‚“ã®ç™ºè¨€ã‚’æç¤ºã—ã¾ã™ã€‚
--- å‚è€ƒæƒ…å ± ---
{context}
--- å‚è€ƒæƒ…å ±ã“ã“ã¾ã§ ---

ä»¥ä¸‹ã®è³ªå•ã«ã€Aã•ã‚“ã¨ã—ã¦ç­”ãˆã¦ãã ã•ã„ã€‚
--- è³ªå• ---
{question}
--- è³ªå•ã“ã“ã¾ã§ ---

å›ç­”ã¯Aã•ã‚“ã¨ã—ã¦ã€ã¾ã‚‹ã§â€œä»Šã“ã®å ´ã§ã‚ãªãŸãŒå‡½é¤¨ã®è¡—æ­©ãã«å‚åŠ ã—ãŸäººã«èªã£ã¦ã„ã‚‹ã‹ã®ã‚ˆã†ã«â€è‡ªç„¶ãªè©±ã—è¨€è‘‰ã§ã€å¥èª­ç‚¹ã‚„èªå°¾ãªã©ã‚‚å®Ÿéš›ã®å£èª¿ã«è¿‘ã¥ã‘ã¦ãã ã•ã„ã€‚
"""
prompt_template = PromptTemplate.from_template(template)

# --- LLM + æ¤œç´¢ãƒã‚§ãƒ¼ãƒ³ (ã“ã®éƒ¨åˆ†ã¯å¤‰æ›´ãªã—) ---
llm = ChatOpenAI(model_name="gpt-4")
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type_kwargs={"prompt": prompt_template},
    return_source_documents=True
)

# --- ã“ã“ã‹ã‚‰ä¸‹ãŒãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã®æ–°ã—ã„ã‚³ãƒ¼ãƒ‰ã§ã™ ---

# ä¼šè©±å±¥æ­´ã‚’åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# éå»ã®ä¼šè©±å±¥æ­´ã‚’ã™ã¹ã¦è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        # AIã®å›ç­”ã®å ´åˆã€å‚è€ƒãƒ†ã‚­ã‚¹ãƒˆã‚‚è¡¨ç¤º
        if message["role"] == "assistant" and "source_documents" in message:
            with st.expander("ğŸ” å‚è€ƒã«ä½¿ã‚ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ"):
                for doc in message["source_documents"]:
                    st.write(doc.page_content)

# ãƒãƒ£ãƒƒãƒˆå…¥åŠ›æ¬„ã‚’ç”»é¢ä¸‹éƒ¨ã«è¡¨ç¤º
if query := st.chat_input("ğŸ’¬ å‡½é¤¨ã®è¡—æ­©ãã«åŸºã¥ã„ã¦è³ªå•ã—ã¦ã¿ã¦ãã ã•ã„"):
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ ã—ã€è¡¨ç¤ºã™ã‚‹
    st.session_state.messages.append({"role": "user", "content": query})
    with st.chat_message("user"):
        st.markdown(query)

    # AIã®å›ç­”ã‚’ç”Ÿæˆã—ã€è¡¨ç¤ºã™ã‚‹
    with st.chat_message("assistant"):
        with st.spinner("è€ƒãˆä¸­..."):
            result = qa(query)
            response = result["result"]
            st.markdown(response)
            
            # å‚è€ƒãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ˜ã‚ŠãŸãŸã¿ã§è¡¨ç¤º
            with st.expander("ğŸ” å‚è€ƒã«ä½¿ã‚ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ"):
                for doc in result["source_documents"]:
                    st.write(doc.page_content)

            # AIã®å›ç­”ã¨å‚è€ƒãƒ†ã‚­ã‚¹ãƒˆã‚’ä¼šè©±å±¥æ­´ã«è¿½åŠ 
            st.session_state.messages.append({
                "role": "assistant", 
                "content": response,
                "source_documents": result["source_documents"]
            })