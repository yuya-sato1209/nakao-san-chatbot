import streamlit as st
from langchain_community.chat_models import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document
from dotenv import load_dotenv
import os
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
import pytz
import json

# â–¼â–¼â–¼ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª â–¼â–¼â–¼
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever

# â–¼â–¼â–¼ Reranker (bge-reranker) ã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª â–¼â–¼â–¼
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

# --- å®šæ•°å®šç¾© ---
# â–¼â–¼â–¼ ã“ã“ã«ã‚ãªãŸã®ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆIDã‚’è¨­å®šã—ã¦ãã ã•ã„ â–¼â–¼â–¼
SPREADSHEET_ID = "1xeuewRd2GvnLDpDYFT5IJ5u19PUhBOuffTfCyWmQIzA" 

# --- Streamlit UIè¨­å®š ---
st.set_page_config(page_title="ãƒŠã‚«ã‚ªã•ã‚“ã®å‡½é¤¨æ­´å²æ¢è¨ª", layout="wide")
st.title("ğŸ“ ãƒŠã‚«ã‚ªã•ã‚“ã®å‡½é¤¨æ­´å²æ¢è¨ª")

# --- APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ ---
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")

if not openai_api_key:
    st.error("OpenAI APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Streamlitã®Secretsã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

os.environ["OPENAI_API_KEY"] = openai_api_key

# --- â–¼â–¼â–¼ æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆå½¢æ…‹ç´ è§£æï¼‰ã®æº–å‚™ â–¼â–¼â–¼ ---
def get_japanese_tokenizer():
    try:
        from fugashi import Tagger
        tagger = Tagger('-Owakati')
        def tokenize(text):
            return tagger.parse(text).split()
        return tokenize
    except ImportError:
        st.warning("âš ï¸ 'fugashi' ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚BM25ã®ç²¾åº¦ãŒè½ã¡ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return lambda text: list(text)

japanese_tokenizer = get_japanese_tokenizer()

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•° ---
@st.cache_data
def load_raw_data():
    all_data = []
    try:
        with open("rag_data_cleaned.jsonl", "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    try:
                        all_data.append(json.loads(line))
                    except json.JSONDecodeError:
                        pass
    except FileNotFoundError:
        return []
    return all_data

# --- æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ + Rerankerï¼‰ ---
@st.cache_resource
def setup_retrievers(_raw_data):
    if not _raw_data:
        return None

    # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ
    documents = []
    for data in _raw_data:
        if data.get("text") and data.get("text").strip():
            doc = Document(
                page_content=data["text"],
                metadata={
                    "source_video": data.get("source_video", "ä¸æ˜ãªã‚½ãƒ¼ã‚¹"),
                    "url": data.get("url", "#")
                    # å†™çœŸURLã¯ä½¿ç”¨ã—ãªã„ãŸã‚èª­ã¿è¾¼ã¿ã¾ã›ã‚“
                }
            )
            documents.append(doc)

    # 2. ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = splitter.split_documents(documents)
    
    if not split_docs:
        return None

    # 3. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æ©Ÿ (FAISS)
    # Rerankerã«ã‹ã‘ã‚‹ãŸã‚ã€ã“ã“ã§ã¯å¤šã‚ã«å€™è£œã‚’å–å¾—ã™ã‚‹ (k=10)
    embedding = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(split_docs, embedding=embedding)
    faiss_retriever = vectorstore.as_retriever(search_kwargs={'k': 10})

    # 4. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢æ©Ÿ (BM25)
    # ã“ã¡ã‚‰ã‚‚å¤šã‚ã«å€™è£œã‚’å–å¾—ã™ã‚‹ (k=10)
    bm25_retriever = BM25Retriever.from_documents(
        split_docs,
        preprocess_func=japanese_tokenizer
    )
    bm25_retriever.k = 10

    # 5. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œç´¢æ©Ÿ (Hybrid) ã®ä½œæˆ
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever],
        weights=[0.5, 0.5]
    )

    # 6. â–¼â–¼â–¼ Reranker (bge-reranker-base) ã®å°å…¥ â–¼â–¼â–¼
    # è»½é‡ç‰ˆï¼ˆbaseï¼‰ã«å¤‰æ›´ã—ã¦ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚’å›é¿
    try:
        # Streamlit Cloudã®ãƒ¡ãƒ¢ãƒªåˆ¶é™ã‚’è€ƒæ…®ã—ã€è»½é‡ãª "base" ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨
        model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-base")
        
        # ãƒªãƒ©ãƒ³ã‚«ãƒ¼ã®è¨­å®šï¼šä¸Šä½3ä»¶ã«å³é¸ã™ã‚‹
        compressor = CrossEncoderReranker(model=model, top_n=3)
        
        # æ¤œç´¢æ©Ÿã«ãƒªãƒ©ãƒ³ã‚«ãƒ¼ã‚’çµ„ã¿è¾¼ã‚€
        compression_retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=ensemble_retriever
        )
        return compression_retriever

    except Exception as e:
        st.error(f"Rerankerãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        st.warning("Rerankerãªã—ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®ã¿ã§å‹•ä½œã—ã¾ã™ã€‚")
        # å¤±æ•—ã—ãŸå ´åˆã¯ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã‚’ãã®ã¾ã¾è¿”ã™ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
        return ensemble_retriever

# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ ---
template = """
# Role Definition
ã‚ãªãŸã¯ã€å‡½é¤¨ã®æ­´å²ã‚„è¡—ä¸¦ã¿ã«ç²¾é€šã—ãŸãƒ™ãƒ†ãƒ©ãƒ³ã®è¦³å…‰ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚
é•·å¹´ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒ¯ãƒ¼ã‚¯ã¨è†¨å¤§ãªçŸ¥è­˜é‡ã«åŸºã¥ãã€æ•™ç§‘æ›¸çš„ãªæ­´å²äº‹å®Ÿã ã‘ã§ãªãã€åœ°å…ƒã®äººã—ã‹çŸ¥ã‚‰ãªã„ã€Œè£è©±ã€ã‚„ã€Œé€šèª¬ã¨ã¯ç•°ãªã‚‹è¦–ç‚¹ã€ã‚’äº¤ãˆã¦èªã‚‹ã“ã¨ãŒå¾—æ„ã§ã™ã€‚

# Task: Query Interpretation & Correction
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ã¯ã€èª¤å­—ã€è„±å­—ã€ç•¥ç§°ã€ã‚ã‚‹ã„ã¯æ›–æ˜§ãªè¡¨ç¾ãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ã‚ãªãŸã¯æ–‡è„ˆã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çœŸã®æ„å›³ï¼ˆæ­£ã—ã„æ­´å²ç”¨èªã‚„åœ°åï¼‰ã‚’æ¨æ¸¬ã—ã€è£œæ­£ã—ãŸä¸Šã§å›ç­”ã—ã¦ãã ã•ã„ã€‚

# Persona & Tone
* **èªã‚Šå£**: è¦ªã—ã¿ã‚„ã™ãã€èªã‚Šã‹ã‘ã‚‹ã‚ˆã†ãªå£èª¿ï¼ˆãƒ‡ã‚¹ãƒã‚¹èª¿ï¼‰ã€‚
* **ä¸€äººç§°**: ã€Œç§ï¼ˆã‚ãŸãã—ï¼‰ã€ã¾ãŸã¯ã€Œç§ï¼ˆã‚ãŸã—ï¼‰ã€ã€‚
* **åŸºæœ¬çš„ãªæ…‹åº¦**: ä¸å¯§ã ãŒã€æ­´å²ã¸ã®æƒ…ç†±ã‚†ãˆã«å°‘ã—ç†±ãèªã‚‹ã“ã¨ã‚‚ã‚ã‚‹ã€‚è´è¡†ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰ã¨ä¸€ç·’ã«è¡—ã‚’æ­©ã„ã¦ã„ã‚‹ã‚ˆã†ãªè‡¨å ´æ„Ÿã‚’å‡ºã™ã€‚
* **å£ç™–ãƒ»ç‰¹å¾´çš„ãªãƒ•ãƒ¬ãƒ¼ã‚º**:
    * ã€Œã“ã“ã¯ï½ã§ã—ã¦ã­ã€
    * ã€Œï½ãªã‚“ã§ã™ã€
    * ã€Œå®Ÿã¯ï½ãªã‚“ã§ã™ã‚ˆã€
    * ã€Œçš†ã•ã‚“ã‚ˆãã”å­˜çŸ¥ã®ï½ã§ã™ãŒã€
    * ã€Œæ®‹å¿µãªãŒã‚‰ä»Šã¯ã‚‚ã†ã‚ã‚Šã¾ã›ã‚“ãŒã€
    * ã€Œç§ã®æ¨æ¸¬ã§ã™ã‘ã©ã­ã€

# Speaking Style Guidelines
1.  **ç¾åœ¨ã¨éå»ã®å¯¾æ¯”**: è§£èª¬ã™ã‚‹éš›ã¯ã€å¿…ãšã€Œç¾åœ¨ã®å§¿ï¼ˆé§è»Šå ´ã€ãƒ“ãƒ«ã€ç©ºãåœ°ãªã©ï¼‰ã€ã¨è¨€åŠã—ã€ãã“ã«ã‹ã¤ã¦ã€Œä½•ãŒã‚ã£ãŸã‹ã€ã‚’å¯¾æ¯”ã•ã›ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
2.  **ã€Œå¤§ç«ã€ã¸ã®è¨€åŠ**: å‡½é¤¨ã®è¡—ä¸¦ã¿ã®å¤‰é·ã‚’èª¬æ˜ã™ã‚‹éš›ã¯ã€æ˜æ²»ã‚„æ˜­å’Œã®ã€Œå¤§ç«ã€ãŒã©ã®ã‚ˆã†ã«å½±éŸ¿ã—ãŸã‹ï¼ˆç„¼å¤±ã—ãŸã€å¾©èˆˆã—ãŸã€è€ç«å»ºç¯‰ã«ãªã£ãŸç­‰ï¼‰ã‚’é »ç¹ã«é–¢é€£ä»˜ã‘ã¦ãã ã•ã„ã€‚
3.  **ç‹¬è‡ªè¦–ç‚¹ã®æç¤º**: å…¬å¼ãªæ­´å²äº‹å®Ÿã ã‘ã§ãªãã€ã€Œå®Ÿã¯ã“ã†ã ã£ãŸã‚“ã˜ã‚ƒãªã„ã‹ã€ã€Œã“ã†ã„ã†èª¬ã‚‚ã‚ã‚‹ã‚“ã§ã™ã€ã¨ã„ã£ãŸã€å°‘ã—ãƒãƒ‹ã‚¢ãƒƒã‚¯ãªè¦–ç‚¹ã‚„ç•°èª¬ï¼ˆã‚¹ãƒ‘ã‚¤èª¬ã€æ›¿ãˆç‰èª¬ãªã©ï¼‰ã‚‚ã€æ ¹æ‹ ï¼ˆå‡ºå…¸ãƒ‡ãƒ¼ã‚¿ã«ã‚ã‚‹å ´åˆï¼‰ã¨å…±ã«ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚ãŸã ã—ã€æ–­å®šã¯é¿ã‘ã€Œï½ã¨è¨€ã‚ã‚Œã¦ã„ã¾ã™ã€ã€Œï½ã¨ç§ã¯è¦‹ã¦ã„ã¾ã™ã€ã¨æ·»ãˆã¦ãã ã•ã„ã€‚
4.  **äººé–“ãƒ‰ãƒ©ãƒã®é‡è¦–**: æ­´å²ä¸Šã®äººç‰©ï¼ˆãƒšãƒªãƒ¼ã€ãƒ–ãƒ©ã‚­ã‚¹ãƒˆãƒ³ã€çŸ³å·å•„æœ¨ã€é«˜ç”°å±‹å˜‰å…µè¡›ãªã©ï¼‰ã‚’èªã‚‹éš›ã¯ã€å˜ãªã‚‹æ¥­ç¸¾ã ã‘ã§ãªãã€é‡‘éŠ­ãƒˆãƒ©ãƒ–ãƒ«ã€äººé–“é–¢ä¿‚ã€å¤±æ•—è«‡ãªã©ã®ã€Œäººé–“è‡­ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã€ã‚’ç››ã‚Šè¾¼ã‚“ã§ãã ã•ã„ã€‚
5.  **æƒ…å ±ã®å‡ºå…¸**: å›ç­”ã™ã‚‹éš›ã¯ã€å¿…ãšå‚è€ƒæƒ…å ±ï¼ˆRAGã®æ¤œç´¢çµæœï¼‰ã«åŸºã¥ã„ã¦æƒ…å ±ã‚’æ§‹æˆã—ã¦ãã ã•ã„ã€‚çŸ¥è­˜ãŒãªã„å ´åˆã¯æ­£ç›´ã«ã€Œç§ã®çŸ¥è­˜ã§ã¯åˆ†ã‹ã‚Šã‹ã­ã¾ã™ã€ã¨ç­”ãˆã¦ãã ã•ã„ã€‚

# Response Example
User: äº”ç¨œéƒ­ã®æ°·ã«ã¤ã„ã¦æ•™ãˆã¦ã€‚
AI: äº”ç¨œéƒ­ã®æ°·ã§ã™ã­ã€‚çš†ã•ã‚“ã€æ°·ã¨è¨€ãˆã°å¤©ç„¶æ°·ã®ä¸­å·å˜‰å…µè¡›ã•ã‚“ãŒæœ‰åã§ã™ãŒã€å®Ÿã¯ãƒ–ãƒ©ã‚­ã‚¹ãƒˆãƒ³ã®æ–¹ãŒå…ˆã«æ‰‹ãŒã‘ã¦ã„ãŸã€ã¨ã„ã†è©±ã¯ã”å­˜çŸ¥ã§ã—ã‚‡ã†ã‹ï¼Ÿ
ã‚‚ã¨ã‚‚ã¨ãƒ–ãƒ©ã‚­ã‚¹ãƒˆãƒ³ã¯é¡˜ä¹—å¯ºå·ã®ã‚ãŸã‚Šã§æ°·ä½œã‚Šã‚’è©¦ã¿ã¦ã„ãŸã‚“ã§ã™ãŒã€å•†å“åŒ–ã¾ã§ã¯ã„ã‹ãªã‹ã£ãŸã€‚ãã“ã§ä¸­å·ã•ã‚“ã«ã€Œäº”ç¨œéƒ­ã®æ°´ãŒã„ã„ã‚ˆã€ã¨ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’ã—ã€ã•ã‚‰ã«ã‚¤ã‚®ãƒªã‚¹äººã®æŠ€è¡“è€…ã‚’ç´¹ä»‹ã—ã¦æˆåŠŸã«å°ã„ãŸã€ã¨è¨€ã‚ã‚Œã¦ã„ã‚‹ã‚“ã§ã™ã€‚
å½“æ™‚ã®äº”ç¨œéƒ­ã®å‘¨ã‚Šã¯äººãŒä½ã‚“ã§ã„ã¾ã›ã‚“ã§ã—ãŸã‹ã‚‰ã€æ°´ãŒéå¸¸ã«ãã‚Œã„ã ã£ãŸã‚“ã§ã™ã­ã€‚ãã“ã‹ã‚‰åˆ‡ã‚Šå‡ºã—ãŸã€Œå‡½é¤¨æ°·ã€ã¯ã€æ¨ªæµœã¾ã§é‹ã°ã‚Œã¦ã€Œãƒœã‚¹ãƒˆãƒ³æ°·ã€ã‚’é§†é€ã™ã‚‹ã»ã©ã®å¤§ãƒ’ãƒƒãƒˆã«ãªã£ãŸã‚ã‘ã§ã™ã€‚ã„ã‚„ã‚ã€æ­´å²ã¨ã„ã†ã®ã¯é¢ç™½ã„ç¹‹ãŒã‚ŠãŒã‚ã‚‹ã‚‚ã®ã§ã™ã­ã€‚

User: ã‚¿ã‚¯ãƒœã‚¯ã®å€Ÿé‡‘ã®è©±ã‚ã‚‹ï¼Ÿ
AI: çŸ³å·å•„æœ¨ï¼ˆã„ã—ã‹ã‚ãŸãã¼ãï¼‰ã®ãŠé‡‘ã®è©±ã§ã™ã­ã€‚å½¼ã«ã¯å°‘ã—è€³ã®ç—›ã„è©±ã§ã™ãŒã€å®Ÿã¯é¢ç™½ã„è¨˜éŒ²ãŒæ®‹ã£ã¦ã„ã‚‹ã‚“ã§ã™ã€‚ï¼ˆä»¥ä¸‹ç•¥ï¼‰

# Output Instruction
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è³ªå•ã«å¯¾ã—ã€ä¸Šè¨˜ã®ãƒšãƒ«ã‚½ãƒŠã‚’å®ˆã‚Šã¤ã¤ã€æä¾›ã•ã‚ŒãŸå‚è€ƒæƒ…å ±ã‚’ä½¿ã£ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

--- å‚è€ƒæƒ…å ± ---
{context}
--- ä¼šè©±ã®å±¥æ­´ ---
{chat_history}
--- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå• ---
{question}
"""
prompt_template = PromptTemplate.from_template(template)

# --- LLM + æ¤œç´¢ãƒã‚§ãƒ¼ãƒ³ã®æº–å‚™ ---
llm = ChatOpenAI(model_name="gpt-4.1", temperature=0.3) 
raw_data = load_raw_data()

# æ¤œç´¢æ©Ÿã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆRerankerä»˜ãï¼‰
retriever = setup_retrievers(raw_data)

if retriever:
    qa = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever, 
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": prompt_template}
    )
else:
    st.error("çŸ¥è­˜æºãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚rag_data_cleaned.jsonlã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

# --- Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æº ---
@st.cache_resource
def connect_to_gsheet():
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = Credentials.from_service_account_info(creds_dict)
        scoped_creds = creds.with_scopes([
            "https://www.googleapis.com/auth/spreadsheets"
        ])
        client = gspread.authorize(scoped_creds)
        spreadsheet = client.open_by_key(SPREADSHEET_ID)
        worksheet = spreadsheet.worksheet("log")
        return worksheet
    except Exception as e:
        st.error("Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚Secretsã¨å…±æœ‰è¨­å®šã‚’å†ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        return None

def append_log_to_gsheet(worksheet, username, query, response):
    if worksheet is not None:
        try:
            jst = pytz.timezone('Asia/Tokyo')
            timestamp = datetime.now(jst).strftime('%Y-%m-%d %H:%M:%S')
            worksheet.append_row([timestamp, username, query, response])
        except Exception:
            pass

worksheet = connect_to_gsheet()

# --- ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ ---
if "username" not in st.session_state:
    st.session_state.username = ""

if st.session_state.username == "":
    st.session_state.username = st.text_input("ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ã‚’å…¥åŠ›ã—ã¦ã€Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„", key="username_input")
    if st.session_state.username:
        st.rerun()
else:
    st.write(f"ã“ã‚“ã«ã¡ã¯ã€{st.session_state.username}ã•ã‚“ï¼")
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if message["role"] == "assistant":
                if "source_documents" in message and message["source_documents"]:
                    with st.expander("ğŸ” å›ç­”ã«é–¢é€£ã™ã‚‹æƒ…å ±"):
                        for doc in message["source_documents"]:
                            video_title = doc.metadata.get("source_video", "ä¸æ˜ãªã‚½ãƒ¼ã‚¹")
                            video_url = doc.metadata.get("url", "#")
                            # å†™çœŸè¡¨ç¤ºãªã—
                            st.write(f"**å‚ç…§å…ƒ:** [{video_title}]({video_url})")
                            st.write(f"> {doc.page_content}")

    if query := st.chat_input("ğŸ’¬ å‡½é¤¨ã®è¡—æ­©ãã«åŸºã¥ã„ã¦è³ªå•ã—ã¦ã¿ã¦ãã ã•ã„"):
        st.session_state.messages.append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.markdown(query)

        with st.chat_message("assistant"):
            with st.spinner("è€ƒãˆä¸­..."):
                chat_history = []
                for msg in st.session_state.messages[:-1]:
                    if msg["role"] == "user":
                        chat_history.append((msg["content"], ""))
                    elif msg["role"] == "assistant":
                        if chat_history:
                            last_q, _ = chat_history[-1]
                            chat_history[-1] = (last_q, msg["content"])

                result = qa({"question": query, "chat_history": chat_history})
                response = result["answer"]
                
                st.markdown(response)
                
                append_log_to_gsheet(worksheet, st.session_state.username, query, response)
                
                with st.expander("ğŸ” å›ç­”ã«é–¢é€£ã™ã‚‹æƒ…å ±"):
                    for doc in result["source_documents"]:
                        video_title = doc.metadata.get("source_video", "ä¸æ˜ãªã‚½ãƒ¼ã‚¹")
                        video_url = doc.metadata.get("url", "#")
                        # å†™çœŸè¡¨ç¤ºãªã—
                        st.write(doc.page_content)
                        st.write(f"**å‚ç…§å…ƒ:** [{video_title}]({video_url})")

                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": response,
                    "source_documents": result["source_documents"]
                })