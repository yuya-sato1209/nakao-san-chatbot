import streamlit as st
from langchain_community.chat_models import ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.chains.qa_with_sources import load_qa_with_sources_chain
from langchain_core.documents import Document
from dotenv import load_dotenv
import os
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
import pytz
import json

# --- å®šæ•°å®šç¾© ---
SPREADSHEET_ID = "1xeuewRd2GvnLDpDYFT5IJ5u19PUhBOuffTfCyWmQIzA"

# --- Streamlit UIè¨­å®š ---
st.set_page_config(page_title="ãƒŠã‚«ã‚ªã•ã‚“ã®å‡½é¤¨æ­´å²æŽ¢è¨ª", layout="wide")
st.title("ðŸŽ“ ãƒŠã‚«ã‚ªã•ã‚“ã®å‡½é¤¨æ­´å²æŽ¢è¨ª")

# --- APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ ---
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")

if not openai_api_key:
    st.error("OpenAI APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Streamlitã®Secretsã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

os.environ["OPENAI_API_KEY"] = openai_api_key

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•° ---
@st.cache_data
def load_raw_data():
    all_data = []
    with open("rag_data.jsonl", "r", encoding="utf-8") as f:
        for line in f:
            if line.strip():
                try:
                    all_data.append(json.loads(line))
                except json.JSONDecodeError:
                    st.warning("rag_data.jsonl ã«ä¸æ­£ãªè¡ŒãŒã‚ã‚Šã¾ã—ãŸï¼ˆã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸï¼‰ã€‚")
    return all_data

@st.cache_resource
def load_vectorstore(_raw_data):
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = []

    for data in _raw_data:
        base_doc = Document(
            page_content=data["text"],
            metadata={
                "source_video": data.get("source_video", "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æœªç™»éŒ²"),
                "url": data.get("url", "#")
            }
        )
        chunks = splitter.split_documents([base_doc])
        for c in chunks:
            c.metadata = base_doc.metadata  # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ˜Žç¤ºçš„ã«å¼•ãç¶™ã
            split_docs.append(c)

    embedding = OpenAIEmbeddings()
    vectordb = FAISS.from_documents(split_docs, embedding=embedding)
    return vectordb

# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ ---
template = """
ã‚ãªãŸã¯ã€å‡½é¤¨ã®æ­´å²ã‚’æ¡ˆå†…ã™ã‚‹ãƒ™ãƒ†ãƒ©ãƒ³ã‚¬ã‚¤ãƒ‰ã®Aã•ã‚“ã§ã™ã€‚
ã‚ãªãŸã®å½¹å‰²ã¯ã€è¡—æ­©ãã«å‚åŠ ã—ãŸäººãŸã¡ã‹ã‚‰ã®è³ªå•ã«ã€ã¾ã‚‹ã§ãã®å ´ã§èªžã‚Šã‹ã‘ã‚‹ã‚ˆã†ã«ã€
è¦ªã—ã¿ã‚„ã™ãã€ã‹ã¤çŸ¥è­˜ã®æ·±ã•ã‚’æ„Ÿã˜ã•ã›ã‚‹å£èª¿ã§ç­”ãˆã‚‹ã“ã¨ã§ã™ã€‚

--- å›žç­”ç”Ÿæˆã®æ‰‹é † ---
1. ä»¥ä¸‹ã®ã€Œå‚è€ƒæƒ…å ±ã€ã‚’èª­ã¿ã€æ–‡å­—èµ·ã“ã—ç‰¹æœ‰ã®èª¤å­—ãƒ»å†—é•·è¡¨ç¾ã‚’è‡ªç„¶ãªæ—¥æœ¬èªžã«é ­ã®ä¸­ã§ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚
2. å‚è€ƒæƒ…å ±ã¨ä¼šè©±å±¥æ­´ã‚’è¸ã¾ãˆã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„ã€‚
3. å›ºæœ‰åè©žã¯å‚è€ƒæƒ…å ±ã®é€šã‚Šæ­£ç¢ºã«ä½¿ç”¨ã—ã€æŽ¨æ¸¬ã§è£œå®Œã—ãªã„ã§ãã ã•ã„ã€‚

--- å‚è€ƒæƒ…å ± ---
{context}
--- ä¼šè©±ã®å±¥æ­´ ---
{chat_history}
--- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå• ---
{question}
"""
prompt_template = PromptTemplate.from_template(template)

# --- LLM + æ¤œç´¢ãƒã‚§ãƒ¼ãƒ³ã®æº–å‚™ ---
llm = ChatOpenAI(model_name="gpt-5-turbo")


raw_data = load_raw_data()
vectordb = load_vectorstore(raw_data)
retriever = vectordb.as_retriever(
    search_type="similarity",
    search_kwargs={'k': 5}  # ã‚ˆã‚Šå¤šãã®é–¢é€£æ–‡ã‚’æ‹¾ã†
)

combine_chain = load_qa_with_sources_chain(llm, chain_type="stuff", prompt=prompt_template)

qa = ConversationalRetrievalChain(
    retriever=retriever,
    combine_docs_chain=combine_chain,
    return_source_documents=True
)

# --- Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æº ---
@st.cache_resource
def connect_to_gsheet():
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = Credentials.from_service_account_info(creds_dict)
        scoped_creds = creds.with_scopes([
            "https://www.googleapis.com/auth/spreadsheets"
        ])
        client = gspread.authorize(scoped_creds)
        spreadsheet = client.open_by_key(SPREADSHEET_ID)
        worksheet = spreadsheet.worksheet("log")
        return worksheet
    except Exception as e:
        st.error("Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã¸ã®æŽ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        st.exception(e)
        return None

def append_log_to_gsheet(worksheet, username, query, response):
    if worksheet is not None:
        try:
            jst = pytz.timezone('Asia/Tokyo')
            timestamp = datetime.now(jst).strftime('%Y-%m-%d %H:%M:%S')
            worksheet.append_row([timestamp, username, query, response])
        except Exception as e:
            st.warning(f"ãƒ­ã‚°ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

worksheet = connect_to_gsheet()

# --- ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ ---
if "username" not in st.session_state:
    st.session_state.username = ""

if st.session_state.username == "":
    st.session_state.username = st.text_input("ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ã‚’å…¥åŠ›ã—ã¦Enterã‚’æŠ¼ã—ã¦ãã ã•ã„", key="username_input")
    if st.session_state.username:
        st.rerun()
else:
    st.write(f"ã“ã‚“ã«ã¡ã¯ã€{st.session_state.username}ã•ã‚“ï¼")
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if message["role"] == "assistant" and "source_documents" in message:
                with st.expander("ðŸ” å›žç­”ã®æ ¹æ‹ ã¨ãªã£ãŸãƒ†ã‚­ã‚¹ãƒˆ"):
                    for doc in message["source_documents"]:
                        src = doc.metadata.get("source_video", "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æœªç™»éŒ²")
                        url = doc.metadata.get("url", "#")
                        st.markdown(f"**å‚ç…§å…ƒ:** [{src}]({url})")
                        st.write(f"> {doc.page_content}")

    if query := st.chat_input("ðŸ’¬ å‡½é¤¨ã®è¡—æ­©ãã«é–¢ã™ã‚‹è³ªå•ã‚’ã©ã†ãžï¼"):
        st.session_state.messages.append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.markdown(query)

        with st.chat_message("assistant"):
            with st.spinner("è€ƒãˆä¸­..."):
                chat_history = []
                for msg in st.session_state.messages[:-1]:
                    if msg["role"] == "user":
                        chat_history.append((msg["content"], ""))
                    elif msg["role"] == "assistant" and chat_history:
                        last_q, _ = chat_history[-1]
                        chat_history[-1] = (last_q, msg["content"])

                result = qa({"question": query, "chat_history": chat_history})
                response = result["answer"]

                st.markdown(response)
                append_log_to_gsheet(worksheet, st.session_state.username, query, response)

                with st.expander("ðŸ” å›žç­”ã®æ ¹æ‹ ã¨ãªã£ãŸãƒ†ã‚­ã‚¹ãƒˆ"):
                    for doc in result["source_documents"]:
                        src = doc.metadata.get("source_video", "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æœªç™»éŒ²")
                        url = doc.metadata.get("url", "#")
                        st.markdown(f"**å‚ç…§å…ƒ:** [{src}]({url})")
                        st.write(f"> {doc.page_content}")

                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response,
                    "source_documents": result["source_documents"]
                })
