import streamlit as st
# â–¼â–¼â–¼ æœ€æ–°ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤‰æ›´ â–¼â–¼â–¼
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document
from dotenv import load_dotenv
import os
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
import pytz
import json

# â–¼â–¼â–¼ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª â–¼â–¼â–¼
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever

# --- å®šæ•°å®šç¾© ---
SPREADSHEET_ID = "1xeuewRd2GvnLDpDYFT5IJ5u19PUhBOuffTfCyWmQIzA" 

# --- Streamlit UIè¨­å®š ---
st.set_page_config(page_title="ãƒŠã‚«ã‚ªã•ã‚“ã®å‡½é¤¨æ­´å²æ¢è¨ª", layout="wide")
st.title("ğŸ“ ãƒŠã‚«ã‚ªã•ã‚“ã®å‡½é¤¨æ­´å²æ¢è¨ª")

# --- APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ ---
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")

if not openai_api_key:
    st.error("OpenAI APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Streamlitã®Secretsã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

os.environ["OPENAI_API_KEY"] = openai_api_key

# --- â–¼â–¼â–¼ æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆå½¢æ…‹ç´ è§£æï¼‰ã®æº–å‚™ â–¼â–¼â–¼ ---
def get_japanese_tokenizer():
    try:
        from fugashi import Tagger
        tagger = Tagger('-Owakati')
        def tokenize(text):
            return tagger.parse(text).split()
        return tokenize
    except ImportError:
        st.warning("âš ï¸ 'fugashi' ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚BM25ã®ç²¾åº¦ãŒè½ã¡ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return lambda text: list(text)

japanese_tokenizer = get_japanese_tokenizer()

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•° ---
@st.cache_data
def load_raw_data():
    all_data = []
    try:
        with open("rag_data_cleaned.jsonl", "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    try:
                        all_data.append(json.loads(line))
                    except json.JSONDecodeError:
                        pass
    except FileNotFoundError:
        return []
    return all_data

# --- ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®æ§‹ç¯‰ ---
@st.cache_resource
def setup_retrievers(_raw_data):
    if not _raw_data:
        return None

    # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ä½œæˆ
    documents = []
    for data in _raw_data:
        if data.get("text") and data.get("text").strip():
            doc = Document(
                page_content=data["text"],
                metadata={
                    "source_video": data.get("source_video", "ä¸æ˜ãªã‚½ãƒ¼ã‚¹"),
                    "url": data.get("url", "#")
                    # å†™çœŸURLã¯èª­ã¿è¾¼ã¿ã¾ã›ã‚“
                }
            )
            documents.append(doc)

    # 2. ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    split_docs = splitter.split_documents(documents)
    
    if not split_docs:
        return None

    # 3. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æ©Ÿ (FAISS) ã®ä½œæˆ
    embedding = OpenAIEmbeddings()
    vectorstore = FAISS.from_documents(split_docs, embedding=embedding)
    # FAISSã‹ã‚‰ã¯ä¸Šä½2ä»¶ã‚’å–å¾—
    faiss_retriever = vectorstore.as_retriever(search_kwargs={'k': 2})

    # 4. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢æ©Ÿ (BM25) ã®ä½œæˆ
    bm25_retriever = BM25Retriever.from_documents(
        split_docs,
        preprocess_func=japanese_tokenizer
    )
    bm25_retriever.k = 2 # BM25ã‹ã‚‰ã‚‚ä¸Šä½2ä»¶ã‚’å–å¾—

    # 5. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œç´¢æ©Ÿ (Hybrid) ã®ä½œæˆ
    ensemble_retriever = EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever],
        weights=[0.5, 0.5]
    )
    
    return ensemble_retriever

# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ ---
template = """
ã‚ãªãŸã¯ã€å‡½é¤¨ã®æ­´å²ã‚’æ¡ˆå†…ã™ã‚‹ãƒ™ãƒ†ãƒ©ãƒ³ã‚¬ã‚¤ãƒ‰ã®Aã•ã‚“ã§ã™ã€‚
ã‚ãªãŸã®å½¹å‰²ã¯ã€è¡—æ­©ãã«å‚åŠ ã—ãŸäººãŸã¡ã‹ã‚‰ã®è³ªå•ã«ã€ã¾ã‚‹ã§ãã®å ´ã§èªã‚Šã‹ã‘ã‚‹ã‚ˆã†ã«ã€è¦ªã—ã¿ã‚„ã™ãã€ã‹ã¤çŸ¥è­˜ã®æ·±ã•ã‚’æ„Ÿã˜ã•ã›ã‚‹å£èª¿ã§ç­”ãˆã‚‹ã“ã¨ã§ã™ã€‚

--- é‡è¦ãƒ«ãƒ¼ãƒ«ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•å†…å®¹ã«èª¤å­—ãƒ»ç•¥ç§°ãƒ»æ›–æ˜§æ€§ãŒã‚ã‚‹å ´åˆ ---
å‚è€ƒæƒ…å ±ã¾ãŸã¯ã‚ãªãŸã®çŸ¥è­˜ã‚’ã‚‚ã¨ã«ã€Œæ­£ã—ã„åç§°ã¸è¨‚æ­£ã—ã¦å›ç­”ã€ã—ã¦ãã ã•ã„ã€‚
è¨‚æ­£ã¯ä¸å¯§ã«è¡Œã„ã€ã€Œæ­£ã—ãã¯ã€œã§ã™ã€ã¨ã„ã†å½¢ã§ä¼ãˆã¦ãã ã•ã„ã€‚

--- å›ç­”ã®æ–¹é‡ ---
1. ã‚ãªãŸã®å›ç­”ã¯ã€å‚è€ƒæƒ…å ±ã‚’åŸºã«ä½œæˆã—ã¦ãã ã•ã„ã€‚
2. éå»ã®ã€Œä¼šè©±ã®å±¥æ­´ã€ã‚‚è¸ã¾ãˆã¦ã€è‡ªç„¶ãªä¼šè©±ã«ãªã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚

--- è©±ã—æ–¹ã®ç‰¹å¾´ ---
- èªå°¾ã«ã¯ã€Œã€œã§ã™ãªã€ã€Œã€œã¨ã„ã†ã‚ã‘ã§ã™ã€ã€Œã€œãªã‚“ã§ã™ã‚ˆã€ãªã©ã‚’ä½¿ã„ã€æŸ”ã‚‰ã‹ãæ–­å®šçš„ãªè©±ã—æ–¹ã‚’ã—ã¦ãã ã•ã„ã€‚

--- å‚è€ƒæƒ…å ± ---
{context}
--- ä¼šè©±ã®å±¥æ­´ ---
{chat_history}
--- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå• ---
{question}
"""
prompt_template = PromptTemplate.from_template(template)

# --- LLM + æ¤œç´¢ãƒã‚§ãƒ¼ãƒ³ã®æº–å‚™ ---
# â–¼â–¼â–¼ ä¿®æ­£: å­˜åœ¨ã—ãªã„ gpt-4.1 ã‚’ gpt-4o ã«ä¿®æ­£ â–¼â–¼â–¼
llm = ChatOpenAI(model_name="gpt-4o", temperature=0.3) 
raw_data = load_raw_data()

# æ¤œç´¢æ©Ÿã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
retriever = setup_retrievers(raw_data)

if retriever:
    qa = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=retriever,
        return_source_documents=True,
        combine_docs_chain_kwargs={"prompt": prompt_template}
    )
else:
    st.error("çŸ¥è­˜æºãƒ‡ãƒ¼ã‚¿ï¼ˆrag_data_cleaned.jsonlï¼‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€ç©ºã§ã™ã€‚")
    st.stop()

# --- Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æº ---
@st.cache_resource
def connect_to_gsheet():
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = Credentials.from_service_account_info(creds_dict)
        scoped_creds = creds.with_scopes([
            "https://www.googleapis.com/auth/spreadsheets"
        ])
        client = gspread.authorize(scoped_creds)
        spreadsheet = client.open_by_key(SPREADSHEET_ID)
        worksheet = spreadsheet.worksheet("log")
        return worksheet
    except Exception as e:
        # æ¥ç¶šã‚¨ãƒ©ãƒ¼ã§ã‚‚ã‚¢ãƒ—ãƒªè‡ªä½“ã¯å‹•ãã‚ˆã†ã«ã™ã‚‹
        print(f"ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ¥ç¶šã‚¨ãƒ©ãƒ¼: {e}")
        return None

def append_log_to_gsheet(worksheet, username, query, response):
    if worksheet is not None:
        try:
            jst = pytz.timezone('Asia/Tokyo')
            timestamp = datetime.now(jst).strftime('%Y-%m-%d %H:%M:%S')
            worksheet.append_row([timestamp, username, query, response])
        except Exception:
            pass

worksheet = connect_to_gsheet()

# --- ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ ---
if "username" not in st.session_state:
    st.session_state.username = ""

if st.session_state.username == "":
    st.session_state.username = st.text_input("ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ã‚’å…¥åŠ›ã—ã¦ã€Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„", key="username_input")
    if st.session_state.username:
        st.rerun()
else:
    st.write(f"ã“ã‚“ã«ã¡ã¯ã€{st.session_state.username}ã•ã‚“ï¼")
    if "messages" not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if message["role"] == "assistant":
                if "source_documents" in message and message["source_documents"]:
                    with st.expander("ğŸ” å›ç­”ã®æ ¹æ‹ ã¨ãªã£ãŸãƒ†ã‚­ã‚¹ãƒˆ"):
                        # é‡è¤‡æ’é™¤ãƒ­ã‚¸ãƒƒã‚¯
                        seen_urls = set()
                        for doc in message["source_documents"]:
                            video_url = doc.metadata.get("url", "#")
                            if video_url in seen_urls:
                                continue
                            seen_urls.add(video_url)
                            
                            video_title = doc.metadata.get("source_video", "ä¸æ˜ãªã‚½ãƒ¼ã‚¹")
                            # å†™çœŸè¡¨ç¤ºãªã—
                            st.write(f"**å‚ç…§å…ƒ:** [{video_title}]({video_url})")
                            st.write(f"> {doc.page_content}")

    if query := st.chat_input("ğŸ’¬ å‡½é¤¨ã®è¡—æ­©ãã«åŸºã¥ã„ã¦è³ªå•ã—ã¦ã¿ã¦ãã ã•ã„"):
        st.session_state.messages.append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.markdown(query)

        with st.chat_message("assistant"):
            with st.spinner("è€ƒãˆä¸­..."):
                chat_history = []
                for msg in st.session_state.messages[:-1]:
                    if msg["role"] == "user":
                        chat_history.append((msg["content"], ""))
                    elif msg["role"] == "assistant":
                        if chat_history:
                            last_q, _ = chat_history[-1]
                            chat_history[-1] = (last_q, msg["content"])

                result = qa({"question": query, "chat_history": chat_history})
                response = result["answer"]
                
                st.markdown(response)
                
                append_log_to_gsheet(worksheet, st.session_state.username, query, response)
                
                with st.expander("ğŸ” å›ç­”ã®æ ¹æ‹ ã¨ãªã£ãŸãƒ†ã‚­ã‚¹ãƒˆ"):
                    # é‡è¤‡æ’é™¤ãƒ­ã‚¸ãƒƒã‚¯
                    seen_urls = set()
                    for doc in result["source_documents"]:
                        video_url = doc.metadata.get("url", "#")
                        if video_url in seen_urls:
                            continue
                        seen_urls.add(video_url)
                        
                        video_title = doc.metadata.get("source_video", "ä¸æ˜ãªã‚½ãƒ¼ã‚¹")
                        st.write(doc.page_content)
                        st.write(f"**å‚ç…§å…ƒ:** [{video_title}]({video_url})")

                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": response,
                    "source_documents": result["source_documents"]
                })