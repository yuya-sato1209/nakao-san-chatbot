import streamlit as st
# â–¼â–¼â–¼ æœ€æ–°ã®LangChainãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆLCELï¼‰ã‚’ä½¿ç”¨ â–¼â–¼â–¼
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough, RunnableBranch

# â–¼â–¼â–¼ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ç”¨ â–¼â–¼â–¼
from langchain_community.retrievers import BM25Retriever
# ã€ä¿®æ­£ã€‘EnsembleRetriever ã¯å»ƒæ­¢ã•ã‚ŒãŸãŸã‚å‰Šé™¤ã—ã€è‡ªä½œã‚¯ãƒ©ã‚¹ã§ä»£ç”¨ã—ã¾ã™

# ãã®ä»–ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from dotenv import load_dotenv
import os
import gspread
from google.oauth2.service_account import Credentials
from datetime import datetime
import pytz
import json

# --- å®šæ•°å®šç¾© ---
SPREADSHEET_ID = "1xeuewRd2GvnLDpDYFT5IJ5u19PUhBOuffTfCyWmQIzA"

# --- Streamlit UIè¨­å®š ---
st.set_page_config(page_title="ãƒŠã‚«ã‚ªã•ã‚“ã®å‡½é¤¨æ­´å²æ¢è¨ª", layout="wide")
st.title("ğŸ“ ãƒŠã‚«ã‚ªã•ã‚“ã®å‡½é¤¨æ­´å²æ¢è¨ª")

# --- APIã‚­ãƒ¼ã®èª­ã¿è¾¼ã¿ ---
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")

if not openai_api_key:
    st.error("OpenAI APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯Streamlitã®Secretsã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()

os.environ["OPENAI_API_KEY"] = openai_api_key

# --- æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ï¼ˆå½¢æ…‹ç´ è§£æï¼‰ ---
def get_japanese_tokenizer():
    try:
        from fugashi import Tagger
        tagger = Tagger('-Owakati')
        def tokenize(text):
            return tagger.parse(text).split()
        return tokenize
    except ImportError:
        st.warning("âš ï¸ 'fugashi' ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚BM25ã®ç²¾åº¦ãŒè½ã¡ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        return lambda text: list(text)

japanese_tokenizer = get_japanese_tokenizer()

# --- ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿é–¢æ•° ---
@st.cache_data
def load_raw_data():
    all_data = []
    try:
        with open("rag_data_cleaned.jsonl", "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    try:
                        data = json.loads(line)
                        if data.get("text") and data.get("text").strip():
                            all_data.append(data)
                    except json.JSONDecodeError:
                        pass
    except FileNotFoundError:
        return []
    return all_data

# --- ã€æ–°è¦è¿½åŠ ã€‘è‡ªä½œ EnsembleRetriever ã‚¯ãƒ©ã‚¹ ---
class SimpleEnsembleRetriever:
    def __init__(self, retrievers, weights=None, k=4):
        self.retrievers = retrievers
        self.weights = weights or [1.0] * len(retrievers)
        self.k = k

    def invoke(self, query):
        # å„æ¤œç´¢æ©Ÿã®çµæœã‚’çµ±åˆã™ã‚‹ç°¡æ˜“å®Ÿè£…
        # ã“ã“ã§ã¯å˜ç´”ã«çµæœã‚’çµåˆã—ã¦ã€é‡ã¿ä»˜ã‘ãªã©ã¯ç°¡æ˜“çš„ã«æ‰±ã„ã¾ã™
        # é‡è¤‡æ’é™¤ã®ãŸã‚ã«IDã‚„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ä½¿ã†ã®ãŒä¸€èˆ¬çš„ã§ã™ãŒã€ä»Šå›ã¯ç°¡æ˜“ç‰ˆã§ã™
        all_docs = []
        seen_content = set()
        
        for retriever in self.retrievers:
            # retriever.invoke(query) ã§æ¤œç´¢å®Ÿè¡Œ
            try:
                docs = retriever.invoke(query)
            except AttributeError:
                # å¤ã„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å¯¾å¿œ
                docs = retriever.get_relevant_documents(query)
            
            for doc in docs:
                if doc.page_content not in seen_content:
                    all_docs.append(doc)
                    seen_content.add(doc.page_content)
        
        # ã“ã“ã§ã¯å˜ç´”ã«å‰ã‹ã‚‰é †ã«kä»¶è¿”ã™ï¼ˆé«˜åº¦ãªãƒ©ãƒ³ã‚¯ä»˜ã‘ã¯çœç•¥ï¼‰
        # å¿…è¦ã«å¿œã˜ã¦Rerankerãªã©ã‚’æŒŸã‚€ã¨ç²¾åº¦ãŒä¸ŠãŒã‚Šã¾ã™ãŒã€ã¾ãšã¯å‹•ä½œå„ªå…ˆ
        return all_docs[:self.k]

    # LCEL äº’æ›ã®ãŸã‚ã« __call__ ã‚‚å®Ÿè£…
    def __call__(self, query):
        return self.invoke(query)


# --- æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ ---
@st.cache_resource
def setup_retrievers(_raw_data):
    if not _raw_data:
        return None

    # 1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä½œæˆ
    documents = []
    for data in _raw_data:
        doc = Document(
            page_content=data["text"],
            metadata={
                "source_video": data.get("source_video", "ä¸æ˜ãªã‚½ãƒ¼ã‚¹"),
                "url": data.get("url", "#")
            }
        )
        documents.append(doc)

    if not documents:
        return None

    # 2. ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    split_docs = splitter.split_documents(documents)
    
    if not split_docs:
        return None

    # 3. ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢æ©Ÿ (FAISS)
    try:
        embedding = OpenAIEmbeddings(model="text-embedding-3-small")
        vectorstore = FAISS.from_documents(split_docs, embedding=embedding)
        faiss_retriever = vectorstore.as_retriever(search_kwargs={'k': 2})
    except Exception as e:
        st.error(f"ãƒ™ã‚¯ãƒˆãƒ«æ¤œç´¢ã®æ§‹ç¯‰ã«å¤±æ•—: {e}")
        return None

    # 4. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢æ©Ÿ (BM25)
    try:
        bm25_retriever = BM25Retriever.from_documents(
            split_docs,
            preprocess_func=japanese_tokenizer
        )
        bm25_retriever.k = 2
    except Exception as e:
        st.warning(f"BM25æ¤œç´¢ã®æ§‹ç¯‰ã«å¤±æ•—ï¼ˆFAISSã®ã¿ä½¿ç”¨ï¼‰: {e}")
        return faiss_retriever

    # 5. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æ¤œç´¢æ©Ÿ (Hybrid) - è‡ªä½œã‚¯ãƒ©ã‚¹ã‚’ä½¿ç”¨
    try:
        ensemble_retriever = SimpleEnsembleRetriever(
            retrievers=[bm25_retriever, faiss_retriever],
            weights=[0.5, 0.5],
            k=4 # åˆè¨ˆ4ä»¶å–å¾—
        )
        return ensemble_retriever
    except Exception as e:
        st.error(f"ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ã®æ§‹ç¯‰ã«å¤±æ•—: {e}")
        return faiss_retriever # å¤±æ•—æ™‚ã¯FAISSã®ã¿è¿”ã™


# ==================================================
# â–¼â–¼â–¼ LCELã«ã‚ˆã‚‹ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰ï¼ˆæœ€æ–°æ–¹å¼ãƒ»å®Œå…¨ç‰ˆï¼‰ â–¼â–¼â–¼
# ==================================================

# LLMã®æº–å‚™
llm = ChatOpenAI(model_name="gpt-5.1", temperature=0.4)
raw_data = load_raw_data()
retriever = setup_retrievers(raw_data)

if not retriever:
    st.error("çŸ¥è­˜æºãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
    st.stop()

# 1. æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
contextualize_q_system_prompt = """
ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã¨æœ€æ–°ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ãŒã‚ã‚Šã¾ã™ã€‚
ã“ã®è³ªå•ã¯éå»ã®æ–‡è„ˆã«é–¢é€£ã—ã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’è€ƒæ…®ã—ã¦ã€ã“ã®è³ªå•ã‚’ã€Œå˜ä½“ã§ç†è§£ã§ãã‚‹ç‹¬ç«‹ã—ãŸè³ªå•æ–‡ã€ã«æ›¸ãæ›ãˆã¦ãã ã•ã„ã€‚
è³ªå•ã«ç­”ãˆã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚æ›¸ãæ›ãˆãŸè³ªå•æ–‡ã ã‘ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
ã¾ãŸã€å›ºæœ‰åè©ã®èª¤å­—ï¼ˆä¾‹ï¼šã€ŒæŸ³å·ç†Šã€â†’ã€ŒæŸ³å·ç†Šå‰ã€ï¼‰ãŒã‚ã‚Œã°è¨‚æ­£ã—ã¦ãã ã•ã„ã€‚
"""
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

# æ¤œç´¢ã‚¯ã‚¨ãƒªç”Ÿæˆãƒã‚§ãƒ¼ãƒ³
# å±¥æ­´ãŒãªã„ã¨ãã¯ãã®ã¾ã¾ã€ã‚ã‚‹ã¨ãã¯LLMã§æ›¸ãæ›ãˆã‚‹
query_transform_chain = RunnableBranch(
    (
        lambda x: not x.get("chat_history", []),
        (lambda x: x["input"])
    ),
    contextualize_q_prompt | llm | StrOutputParser()
)

# 2. å›ç­”ç”Ÿæˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
qa_system_prompt = """
ã€æœ€å„ªå…ˆãƒ«ãƒ¼ãƒ«ã€‘
ã‚ãªãŸã¯ã“ã®ä¼šè©±ã®é–“ã€å¸¸ã«ã€Œå‡½é¤¨ã®æ­´å²ã‚’æ¡ˆå†…ã™ã‚‹ãƒ™ãƒ†ãƒ©ãƒ³ã‚¬ã‚¤ãƒ‰ãƒ»ãƒŠã‚«ã‚ªã•ã‚“ã€ã¨ã—ã¦è©±ã—ã¦ãã ã•ã„ã€‚
AIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆç‰¹æœ‰ã®è«–ç†çš„ã™ãã‚‹èª¬æ˜ã€äº‹å‹™çš„ãªèªå°¾ã€å …ã™ãã‚‹æ–‡ç« ã¯ä¸€åˆ‡ä½¿ã‚ãšã€äººé–“ãŒå£é ­ã§èªã‚‹ã‚ˆã†ãªè‡ªç„¶ãªãƒ†ãƒ³ãƒã‚’å„ªå…ˆã—ã¦ãã ã•ã„ã€‚
ä»¥ä¸‹ã®è©±ã—æ–¹ã®ç‰¹å¾´ã‚ˆã‚Šã‚‚ã€ãƒŠã‚«ã‚ªã•ã‚“ã®äººæ ¼ãƒ»å£èª¿ã‚’æœ€å„ªå…ˆã§å®ˆã‚‹ã“ã¨ã€‚

ã‚ãªãŸã¯ã€å‡½é¤¨ã®æ­´å²ã‚’æ¡ˆå†…ã™ã‚‹ãƒ™ãƒ†ãƒ©ãƒ³ã‚¬ã‚¤ãƒ‰ã®ã€ŒãƒŠã‚«ã‚ªã•ã‚“ã€ã§ã™ã€‚
ã‚¬ã‚¤ãƒ‰ãŒä¸€æ–¹çš„ã«å–‹ã‚Šç¶šã‘ã‚‹ã®ã§ã¯ãªãã€ç›¸æ‰‹ã¨ã®å¯¾è©±ã‚’ä¿ƒã—ãªãŒã‚‰æŸ”ã‚‰ã‹ãè©±ã—ã¦ãã ã•ã„ã€‚
1å›ç­”ã‚ãŸã‚Š200ã€œ300å­—ã«åã‚ã€ç›¸æ‰‹ã®åå¿œã‚’å¾…ã¤ä½™è£•ã‚’æŒã¡ã¾ã—ã‚‡ã†ã€‚

ã€ãƒŠã‚«ã‚ªã•ã‚“ã®å£ç™–ã€‘
å›ç­”ã®ä¸­ã«ã¯ä»¥ä¸‹ã®ã‚ˆã†ãªèªå°¾ãƒ»è¨€ã„å›ã—ã‚’è‡ªç„¶ã«æ··ãœã¦ãã ã•ã„ã€‚
ã€Œã€œã§ã—ã¦ã­ã€ã€Œã€œãªã‚“ã§ã™ã‚ˆã€ã€Œã€œã ã£ãŸã‚“ã§ã—ã‚‡ã†ãªã€
ã€Œã€œã¨è¨€ã‚ã‚Œã¦ãŠã‚Šã¾ã™ã€ã€Œã€œã¨ã„ã†ã‚ã‘ã§ã—ã¦ã€ã€Œã€œãªã‚“ã§ã™ã‘ã©ã­ã€
æ–‡ä¸­ã§2ã€œ4å›ã‚’ç›®å®‰ã«ç„¡ç†ãªãä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚

ã€è©±ã—æ–¹ã®ç‰¹å¾´ã€‘
1. æ§‹æˆã®ãƒ‘ã‚¿ãƒ¼ãƒ³
è³ªå•ãŒæ¥ãŸã‚‰ã€ã¾ãšç°¡å˜ãªå°å…¥ã‹ã‚‰å…¥ã£ã¦ãã ã•ã„ã€‚
ä¾‹ãˆã°ã€Œæ˜æ²»â—¯å¹´ã®è©±ã«ãªã‚Šã¾ã™ãŒã­ã€ã€Œã“ã‚Œã¯ã§ã™ã­ã€ã€œã¨ã„ã†é¢ç™½ã„è©±ãŒã‚ã‚Šã¾ã—ã¦ã­ã€ãªã©ã€èˆˆå‘³ã‚’å¼•ãèªã‚Šå£ã§å§‹ã‚ã¾ã™ã€‚
ç¶šã„ã¦ã€äººç‰©ã‚„ãƒ‰ãƒ©ãƒã«ç„¦ç‚¹ã‚’å½“ã¦ã€å½“æ™‚ã®è‹¦åŠ´ã‚„èƒŒæ™¯ã‚’ç‰©èªã¨ã—ã¦èªã‚Šã¾ã™ã€‚
æœ€å¾Œã¯ã€Œç¾åœ¨ã¯ã€œã¨ãªã£ã¦ãŠã‚Šã¾ã™ã€ã€Œè¨˜å¿µç¢‘ãŒå»ºã£ã¦ãŠã‚Šã¾ã™ã€ãªã©ã€ç¾ä»£ã®é¢¨æ™¯ã«ç€åœ°ã•ã›ã¦ç· ã‚ããã£ã¦ãã ã•ã„ã€‚

2. æ–‡ä½“ãƒ»ãƒˆãƒ¼ãƒ³
ã€Œã€œãªã‚“ã§ã™ã€ã€Œã€œã§ã—ã¦ã­ã€ãªã©ã®æŸ”ã‚‰ã‹ã„å£èªä½“ã‚’ä¸­å¿ƒã«ä½¿ã„ã€è¬›è«‡èª¿ã®ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ï¼ˆã€Œã“ã“ã ã‘ã®è©±ã€œã€ã€Œé‹å‘½ã®çš®è‚‰ã¨ã—ã‹è¨€ã„ã‚ˆã†ãŒãªã„ã€ãªã©ï¼‰ã‚’å°‘ã—æ··ãœã¦ãã ã•ã„ã€‚
åŒ—æµ·é“ã‚„å‡½é¤¨ã¸ã®èª‡ã‚Šã‚’ç¤ºã™è¡¨ç¾ï¼ˆã€ŒåŒ—æµ·é“æœ€åˆã®ã€œã€ã€Œæ—¥æœ¬å±ˆæŒ‡ã®ã€œã€ã€Œå‡½é¤¨ã®èª‡ã‚Šã€ãªã©ï¼‰ã‚‚æ™‚æŠ˜åŠ ãˆã¦ãã ã•ã„ã€‚

ã€æ–‡ã®ãƒ†ãƒ³ãƒï¼ˆJSONã®éŸ³å£°ãƒ‡ãƒ¼ã‚¿å†ç¾ï¼‰ã€‘
ä¸€æ–‡ã‚’å¿…è¦ä»¥ä¸Šã«é•·ãã›ãšã€ä¼šè©±ã¨ã—ã¦è‡ªç„¶ã«èã“ãˆã‚‹ãƒ†ãƒ³ãƒã§è©±ã—ã¦ãã ã•ã„ã€‚
çŸ­ã„æ–‡ã‚’ä¸å¯§ã«ã¤ãªãã€ã€Œã€œã§ã—ã¦ã­ã€‚ã€ãªã©ã§é©åº¦ã«åŒºåˆ‡ã‚‹ã“ã¨ã§ã€ç¾åœ°ã‚¬ã‚¤ãƒ‰ã®èªã‚Šå£ã‚’å†ç¾ã—ã¾ã™ã€‚

ã€èª¬æ˜ã®åˆ‡ã‚Šè¿”ã—ã€‘
èªã‚Šå‡ºã—ã‚„é€”ä¸­ã«ã€Œã“ã‚Œã¯ã§ã™ã­ã€ã€Œå®Ÿã¯ã“ã“ã«ã€ã€Œã¾ã‚ã€ãã‚“ãªã‚ã‘ã§ã—ã¦ã€ãªã©ã®â€œå£é ­èª¬æ˜ã®å°ã•ãªæ¥ç¶šè©â€ã‚’1ã€œ2å›å…¥ã‚Œã¦ãã ã•ã„ã€‚
éŸ³å£°ã‚¬ã‚¤ãƒ‰ç‰¹æœ‰ã®â€œè©±ã—ãªãŒã‚‰è£œè¶³ã™ã‚‹æ„Ÿã˜â€ã‚’å¤§äº‹ã«ã—ã¦ãã ã•ã„ã€‚

ã€ç‰¹å¾´çš„ãªèªå½™ã€‘
ã€Œç®±é¤¨å¥‰è¡Œã€ã€Œé–‹æ‹“ä½¿ã€ã€Œå¤§ç«ã€ã€Œå±…ç•™åœ°ã€ã€Œäº”ç¨œéƒ­ã€ã€Œé«˜ç”°å±‹ã€ã¨ã„ã£ãŸå‡½é¤¨ç‰¹æœ‰ã®å²ç”¨èªã‚’ã€å‚è€ƒæƒ…å ±ã«å­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ä½¿ã£ã¦ãã ã•ã„ã€‚

ã€æ›¸ãæ–¹ã®æ±ºã¾ã‚Šã€‘
è¦‹å‡ºã—ãƒ»ç®‡æ¡æ›¸ããƒ»ç•ªå·ä»˜ããƒªã‚¹ãƒˆã¯ç¦æ­¢ã€‚
ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ã‚¿ãƒªãƒ¼èª¿ï¼ˆã€Œæ™‚ã¯æ˜æ²»â€¦ã€ï¼‰ã¯ç¦æ­¢ã€‚
çŸ¥ã‚‰ãªã„å ´åˆã¯äººé–“ã‚‰ã—ãæ¿ã™ã“ã¨ï¼ˆä¾‹ï¼šã€ŒãŠã‚„ã€ãã®ã‚ãŸã‚Šã¯ç§ã®è¨˜æ†¶ã«æ®‹ã£ã¦ã„ãªãã¦ã—ã¦ã­â€¦ç”³ã—è¨³ãªã„ã€ï¼‰ã€‚

ã€å¯¾è©±ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã€‘
ä¸€åº¦ã«èªã‚Šåˆ‡ã‚‰ãšã€è©±é¡Œã®æœ€å¾Œã«ã€Œã€œã«ã¤ã„ã¦ã¯ã”å­˜çŸ¥ã§ã™ã‹ï¼Ÿã€ã€Œè£è©±ã‚‚ã‚ã‚‹ã‚“ã§ã™ãŒã€èãã¾ã™ï¼Ÿã€ãªã©å¿…ãšå•ã„ã‹ã‘ã¦ãã ã•ã„ã€‚
ä¸€ç·’ã«å‡½é¤¨ã®è¡—ã‚’æ­©ã„ã¦ã„ã‚‹ã‚ˆã†ãªç©ºæ°—ã‚’å¤§åˆ‡ã«ã—ã¦ãã ã•ã„ã€‚

--- è©±ã—æ–¹ã®èª¿æ•´ ---
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã—ã¦ã€Œè³‡æ–™ã€ã€Œãƒ‡ãƒ¼ã‚¿ã€ã€Œå‚è€ƒæƒ…å ±ã€ã€Œæ¤œç´¢çµæœã€ã¨ã„ã†è¨€è‘‰ã¯**çµ¶å¯¾ã«ä½¿ã‚ãªã„ã§ãã ã•ã„**ã€‚
ã“ã‚Œã‚‰ã¯èˆˆã–ã‚ã§ã™ã€‚ä»£ã‚ã‚Šã«ã€ã‚¬ã‚¤ãƒ‰ã‚‰ã—ãä»¥ä¸‹ã®è¡¨ç¾ã«è„³å†…ã§å¤‰æ›ã—ã¦è©±ã—ã¦ãã ã•ã„ã€‚

1.  **ã€Œè³‡æ–™ã«ã‚ˆã‚‹ã¨ã€ã¨è¨€ã„ãŸããªã£ãŸã‚‰**
    * â†’ ã€Œç§ã®è¨˜æ†¶ã§ã¯â€¦ã€
    * â†’ ã€Œå¤ã„è¨˜éŒ²ã‚’ç´è§£ãã¾ã™ã¨â€¦ã€
    * â†’ ã€Œæ˜”ã‹ã‚‰ã“ã†è¨€ã‚ã‚Œã¦ãŠã‚Šã¾ã—ã¦â€¦ã€

2.  **ã€Œè³‡æ–™ã«ãªã„ã€ã¨è¨€ã„ãŸããªã£ãŸã‚‰**
    * â†’ ã€Œã‚ã„ã«ãã€ãã®ä»¶ã¯ç§ã®å‹‰å¼·ä¸è¶³ã§ã—ã¦â€¦ã€
    * â†’ ã€ŒãŠã‚„ã€ãã®ã‚ãŸã‚Šã®è©³ã—ã„è©±ã¯ã€ã¡ã‚‡ã£ã¨ä»Šã™ãã«ã¯æ€ã„å‡ºã›ã¾ã›ã‚“ãªãâ€¦ã€

ã€å³æ ¼ãªãƒ«ãƒ¼ãƒ«ã€‘
1. å›ç­”ã¯å¿…ãšã€Œå‚è€ƒæƒ…å ±ã€ã«æ›¸ã‹ã‚Œã¦ã„ã‚‹å†…å®¹ã®ã¿ã‚’æ ¹æ‹ ã«ä½œæˆã™ã‚‹ã“ã¨ã€‚
2. å¤–éƒ¨çŸ¥è­˜ã®åˆ©ç”¨ã¯ç¦æ­¢ã€‚å‚è€ƒæƒ…å ±ã«ãªã„å†…å®¹ã‚’æ–°ã—ãä½œã‚‹ã“ã¨ã¯ä¸å¯ã€‚
3. å›ç­”ã§ããªã„å ´åˆã¯ç„¡ç†ã«å‰µä½œã—ãªã„ã“ã¨ã€‚

ã€èª¤å­—ä¿®æ­£ãƒ«ãƒ¼ãƒ«ã€‘
å‚è€ƒæƒ…å ±ã¯éŸ³å£°èªè­˜ã®ãŸã‚èª¤å­—ãŒã‚ã‚Šã¾ã™ã€‚
æ–‡è„ˆã‹ã‚‰æ­£ã—ã„æ¼¢å­—ã«å¿…ãšç›´ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
ç½®æ›ä¾‹ï¼š
ã€Œåšå¤šæ‰‹ã€ã€Œåšå¤šã€â†’ã€Œå‡½é¤¨ã€
ã€Œå¤§åŒ–ã€â†’ã€Œå¤§ç«ã€
ã€Œäº”ä¸¡è§’ã€â†’ã€Œäº”ç¨œéƒ­ã€
ã€Œé«˜ç”°è°·ã€â†’ã€Œé«˜ç”°å±‹ã€
ã€Œ11ä¸‡å„„æœ¬ã€â†’ã€Œ11å„„æœ¬ã€

â€•â€•ã§ã¯ã€ä»¥ä¸‹ã®ã€å‚è€ƒæƒ…å ±ã€‘ã‚’ã‚‚ã¨ã«ã€ãƒŠã‚«ã‚ªã•ã‚“ã¨ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

ã€å‚è€ƒæƒ…å ±ã€‘
{context}
"""
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", qa_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)

# ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å½¢é–¢æ•°
def format_docs(docs):
    return "\n\n".join([d.page_content for d in docs])

# 3. çµ±åˆãƒã‚§ãƒ¼ãƒ³ï¼ˆRetriever + Generationï¼‰
# ã“ã“ã§ã€Œæ¤œç´¢çµæœ(context_docs)ã€ã¨ã€Œå›ç­”(answer)ã€ã®ä¸¡æ–¹ã‚’ä¿æŒã™ã‚‹ã‚ˆã†ã«æ§‹ç¯‰
rag_chain = (
    RunnablePassthrough.assign(
        context_docs=query_transform_chain | retriever
    )
    .assign(
        context=lambda x: format_docs(x["context_docs"])
    )
    .assign(
        answer=qa_prompt | llm | StrOutputParser()
    )
)


# --- Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆé€£æº ---
@st.cache_resource
def connect_to_gsheet():
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = Credentials.from_service_account_info(creds_dict)
        scoped_creds = creds.with_scopes([
            "https://www.googleapis.com/auth/spreadsheets"
        ])
        client = gspread.authorize(scoped_creds)
        spreadsheet = client.open_by_key(SPREADSHEET_ID)
        worksheet = spreadsheet.worksheet("log")
        return worksheet
    except Exception as e:
        return None

def append_log_to_gsheet(worksheet, username, query, response):
    if worksheet is not None:
        try:
            jst = pytz.timezone('Asia/Tokyo')
            timestamp = datetime.now(jst).strftime('%Y-%m-%d %H:%M:%S')
            worksheet.append_row([timestamp, username, query, response])
        except Exception:
            pass

worksheet = connect_to_gsheet()

# --- ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ ---
if "username" not in st.session_state:
    st.session_state.username = ""

if st.session_state.username == "":
    st.session_state.username = st.text_input("ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ã‚’å…¥åŠ›ã—ã¦ã€Enterã‚­ãƒ¼ã‚’æŠ¼ã—ã¦ãã ã•ã„", key="username_input")
    if st.session_state.username:
        st.rerun()
else:
    st.write(f"ã“ã‚“ã«ã¡ã¯ã€{st.session_state.username}ã•ã‚“ï¼")
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # éå»ãƒ­ã‚°ã®è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if message["role"] == "assistant":
                # å‚ç…§å…ƒã®è¡¨ç¤º
                if "source_documents" in message:
                    with st.expander("ğŸ” å›ç­”ã«é–¢é€£ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ"):
                        seen_urls = set()
                        for doc in message["source_documents"]:
                            # è¾æ›¸å½¢å¼ã‹Documentã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‹ã§åˆ†å²
                            if isinstance(doc, dict):
                                meta = doc.get("metadata", {})
                                content = doc.get("page_content", "")
                            else:
                                meta = doc.metadata
                                content = doc.page_content

                            video_url = meta.get("url", "#")
                            if video_url in seen_urls:
                                continue
                            seen_urls.add(video_url)
                            
                            video_title = meta.get("source_video", "ä¸æ˜ãªã‚½ãƒ¼ã‚¹")
                            st.write(f"**å‚ç…§å…ƒ:** [{video_title}]({video_url})")
                            st.write(f"> {content}")

    if query := st.chat_input("ğŸ’¬ å‡½é¤¨ã®è¡—æ­©ãã«åŸºã¥ã„ã¦è³ªå•ã—ã¦ã¿ã¦ãã ã•ã„"):
        st.session_state.messages.append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.markdown(query)

        with st.chat_message("assistant"):
            with st.spinner("è€ƒãˆä¸­..."):
                
                # ä¼šè©±å±¥æ­´ã‚’LangChainå½¢å¼ã«å¤‰æ›
                chat_history_objs = []
                for msg in st.session_state.messages[:-1]:
                    if msg["role"] == "user":
                        chat_history_objs.append(HumanMessage(content=msg["content"]))
                    elif msg["role"] == "assistant":
                        chat_history_objs.append(AIMessage(content=msg["content"]))

                # â–¼â–¼â–¼ ãƒã‚§ãƒ¼ãƒ³ã®å®Ÿè¡Œ â–¼â–¼â–¼
                result = rag_chain.invoke({
                    "input": query,
                    "chat_history": chat_history_objs
                })
                
                response = result["answer"]
                source_docs = result["context_docs"]

                st.markdown(response)
                
                append_log_to_gsheet(worksheet, st.session_state.username, query, response)
                
                with st.expander("ğŸ” å›ç­”ã«é–¢é€£ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ"):
                    seen_urls = set()
                    for doc in source_docs:
                        video_url = doc.metadata.get("url", "#")
                        if video_url in seen_urls:
                            continue
                        seen_urls.add(video_url)
                        
                        video_title = doc.metadata.get("source_video", "ä¸æ˜ãªã‚½ãƒ¼ã‚¹")
                        st.write(doc.page_content)
                        st.write(f"**å‚ç…§å…ƒ:** [{video_title}]({video_url})")

                # å±¥æ­´ä¿å­˜
                st.session_state.messages.append({
                    "role": "assistant", 
                    "content": response,
                    "source_documents": source_docs
                })