# app.py
import os
import json
from datetime import datetime
import pytz

import streamlit as st
from dotenv import load_dotenv

# LangChain / community
from langchain_community.chat_models import ChatOpenAI
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import ConversationalRetrievalChain

# Hybrid retriever
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever

# Google Sheets
import gspread
from google.oauth2.service_account import Credentials

# -------------------------
# è¨­å®š
# -------------------------
SPREADSHEET_ID = "1xeuewRd2GvnLDpDYFT5IJ5u19PUhBOuffTfCyWmQIzA"
DATA_JSONL = "rag_data_cleaned.jsonl"  # JSONL ãƒ•ã‚¡ã‚¤ãƒ«å
CHUNK_SIZE = 700
CHUNK_OVERLAP = 120

# -------------------------
# Streamlit UI åŸºæœ¬
# -------------------------
st.set_page_config(page_title="ãƒŠã‚«ã‚ªã•ã‚“ã®å‡½é¤¨æ­´å²æ¢è¨ªï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ç‰ˆï¼‰", layout="wide")
st.title("ğŸ“ ãƒŠã‚«ã‚ªã•ã‚“ã®å‡½é¤¨æ­´å²æ¢è¨ªï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ¤œç´¢ + GPT-5ï¼‰")

# -------------------------
# OpenAI APIã‚­ãƒ¼
# -------------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY") or st.secrets.get("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    st.error("OpenAI APIã‚­ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚.env ã¾ãŸã¯ Streamlit Secrets ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
    st.stop()
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# -------------------------
# æ—¥æœ¬èªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶æº–å‚™ï¼ˆfugashi / MeCabï¼‰ - BM25 ç”¨
# -------------------------
try:
    from fugashi import Tagger
    tagger = Tagger()
    def japanese_tokenize(text: str) -> str:
        # MeCab ã§ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®å½¢ã«å¤‰æ›ï¼ˆBM25ç”¨ï¼‰
        tokens = [word.surface for word in tagger(text)]
        return " ".join(tokens)
    st.write("ğŸ” fugashi (MeCab) tokenizer: OK")
except Exception as e:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆå˜ç´”åˆ†å‰²ï¼‰ â€” ç²¾åº¦ã¯è½ã¡ã‚‹
    tagger = None
    def japanese_tokenize(text: str) -> str:
        # ç°¡æ˜“: å¥ç‚¹ãƒ»ç©ºç™½ã§åˆ†å‰²ã—ã¦ã‚¹ãƒšãƒ¼ã‚¹ã‚’å…¥ã‚Œã‚‹ï¼ˆç²¾åº¦è½ã¡ï¼‰
        s = text.replace("\n", " ")
        # insert spaces between Kanji/Hiragana/Katakana and ASCII sequences crudely
        return " ".join([t for t in s.split() if t])
    st.warning("âš  fugashi (MeCab) ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚BM25 ã®ç²¾åº¦ãŒè½ã¡ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

# -------------------------
# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
# -------------------------
@st.cache_data
def load_raw_data(path=DATA_JSONL):
    all_data = []
    try:
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                if line.strip():
                    try:
                        all_data.append(json.loads(line))
                    except json.JSONDecodeError:
                        # skip invalid lines
                        continue
    except FileNotFoundError:
        return []
    return all_data

raw_data = load_raw_data()

# -------------------------
# ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ + BM25 æ§‹ç¯‰ï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ£ãƒ³ã‚¯ã«æ˜ç¤ºçš„ã«ã‚³ãƒ”ãƒ¼ï¼‰
# -------------------------
@st.cache_resource
def build_retrievers(_raw_data):
    if not _raw_data:
        return None

    # 1) Document ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å«ã‚€ï¼‰
    documents = []
    for item in _raw_data:
        text = item.get("text", "").strip()
        if not text:
            continue
        doc = Document(
            page_content=text,
            metadata={
                "source_video": item.get("source_video", "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æœªç™»éŒ²"),
                "url": item.get("url", "#")
            }
        )
        documents.append(doc)

    if not documents:
        return None

    # 2) ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ï¼ˆå…ƒãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼ï¼‰
    splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)
    chunked_docs = []
    for doc in documents:
        chunk_texts = splitter.split_text(doc.page_content)
        for t in chunk_texts:
            chunked_docs.append(Document(page_content=t, metadata=doc.metadata.copy()))

    # 3) FAISS (æ„å‘³æ¤œç´¢)
    embeddings = OpenAIEmbeddings()
    faiss_db = FAISS.from_documents(chunked_docs, embedding=embeddings)
    faiss_retriever = faiss_db.as_retriever(search_kwargs={"k": 4})

    # 4) BM25 (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢) â€” æ—¥æœ¬èªã‚’äº‹å‰ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦ä¸ãˆã‚‹
    # BM25 ã¯å˜èªå¢ƒç•ŒãŒé‡è¦ãªã®ã§ã€å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¦ãŠã
    bm25_documents_for_index = []
    for d in chunked_docs:
        tokenized = japanese_tokenize(d.page_content)
        # BM25Retriever.from_documents expects Documents; we feed tokenized text as page_content
        bm25_documents_for_index.append(Document(page_content=tokenized, metadata=d.metadata.copy()))

    bm25_retriever = BM25Retriever.from_documents(bm25_documents_for_index)
    bm25_retriever.k = 3  # BM25 ã‹ã‚‰ä¸Šä½3ä»¶ã‚’å–ã‚‹

    # 5) Ensemble (é‡ã¿: BM25 0.3, FAISS 0.7)
    ensemble = EnsembleRetriever(
        retrievers=[bm25_retriever, faiss_retriever],
        weights=[0.3, 0.7]
    )

    return {
        "faiss_db": faiss_db,
        "faiss_retriever": faiss_retriever,
        "bm25_retriever": bm25_retriever,
        "ensemble_retriever": ensemble
    }

retrievers = build_retrievers(raw_data)
if not retrievers:
    st.error("çŸ¥è­˜æºãƒ‡ãƒ¼ã‚¿ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚rag_data_cleaned.jsonl ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
    st.stop()

ensemble_retriever = retrievers["ensemble_retriever"]

# -------------------------
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆèª¤å­—è¨‚æ­£ãƒ»å‚è€ƒæƒ…å ±å„ªå…ˆã®ãƒ«ãƒ¼ãƒ«ã‚’æ˜ç¢ºã«ï¼‰
# -------------------------
template = """
ã‚ãªãŸã¯ã€å‡½é¤¨ã®æ­´å²ã‚’æ¡ˆå†…ã™ã‚‹ãƒ™ãƒ†ãƒ©ãƒ³ã‚¬ã‚¤ãƒ‰ã®Aã•ã‚“ã§ã™ã€‚
ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ã€è¦ªã—ã¿ã‚„ã™ãæ·±ã¿ã®ã‚ã‚‹å£èª¿ã§ç­”ãˆã¦ãã ã•ã„ã€‚

--- é‡è¦ãƒ«ãƒ¼ãƒ« ---
1) å‚è€ƒæƒ…å ±ï¼ˆcontextï¼‰ã‚’æœ€å„ªå…ˆã«ã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚å‚è€ƒæƒ…å ±ã«çŸ›ç›¾ãŒã‚ã‚‹å ´åˆã¯ã€å‚è€ƒæƒ…å ±ã‚’æ ¹æ‹ ã«ã—ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
2) ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«èª¤å­—ãƒ»ç•¥ç§°ãƒ»è¡¨è¨˜ã‚†ã‚ŒãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€å‚è€ƒæƒ…å ±ã¨ã‚ãªãŸã®çŸ¥è­˜ã«åŸºã¥ã„ã¦**æ­£ã—ã„åç§°ã¸è¨‚æ­£ã—ã¦å›ç­”**ã—ã¦ãã ã•ã„ã€‚
   è¨‚æ­£ã¯ä¸å¯§ã«è¡Œã„ã€ã€Œæ­£ã—ãã¯ã€œã§ã™ã€ã®å½¢ã§ä¼ãˆã¦ãã ã•ã„ã€‚
3) å‚è€ƒæƒ…å ±ã®ä¸­ã§æ˜ã‚‰ã‹ã«é–¢ä¿‚ãªã„ã‚‚ã®ã¯ç„¡è¦–ã—ã¦ã€æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æƒ…å ±ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

--- è©±ã—æ–¹ã®ç‰¹å¾´ ---
èªå°¾ã«ã¯ã€Œã€œã§ã™ãªã€ã€Œã€œã¨ã„ã†ã‚ã‘ã§ã™ã€ã€Œã€œãªã‚“ã§ã™ã‚ˆã€ãªã©ã‚’ä½¿ã„ã€æŸ”ã‚‰ã‹ãæ–­å®šçš„ã«è©±ã—ã¦ãã ã•ã„ã€‚

--- å‚è€ƒæƒ…å ± ---
{context}

--- ä¼šè©±ã®å±¥æ­´ ---
{chat_history}

--- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå• ---
{question}
"""
prompt_template = PromptTemplate.from_template(template)

# -------------------------
# LLM ã¨ãƒã‚§ãƒ¼ãƒ³æ§‹ç¯‰ï¼ˆGPT-5 ã‚’æŒ‡å®šï¼‰
# -------------------------
llm = ChatOpenAI(model_name="gpt-5-turbo", temperature=0.2)

combine_chain = create_stuff_documents_chain(
    llm=llm,
    prompt=prompt_template
)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=ensemble_retriever,
    combine_docs_chain=combine_chain,
    return_source_documents=True
)

# -------------------------
# Google ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ¥ç¶šï¼ˆãƒ­ã‚°ç”¨ï¼‰
# -------------------------
@st.cache_resource
def connect_to_gsheet():
    try:
        creds_dict = st.secrets["gcp_service_account"]
        creds = Credentials.from_service_account_info(creds_dict)
        scoped_creds = creds.with_scopes(["https://www.googleapis.com/auth/spreadsheets"])
        client = gspread.authorize(scoped_creds)
        spreadsheet = client.open_by_key(SPREADSHEET_ID)
        worksheet = spreadsheet.worksheet("log")
        return worksheet
    except Exception as e:
        st.warning("Googleã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆæ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆãƒ­ã‚°ã¯ç„¡åŠ¹ã«ãªã‚Šã¾ã™ï¼‰ã€‚")
        return None

worksheet = connect_to_gsheet()

def append_log_to_gsheet(worksheet, username, query, response):
    if worksheet is None:
        return
    try:
        jst = pytz.timezone('Asia/Tokyo')
        timestamp = datetime.now(jst).strftime('%Y-%m-%d %H:%M:%S')
        worksheet.append_row([timestamp, username, query, response])
    except Exception:
        pass

# -------------------------
# ãƒãƒ£ãƒƒãƒˆ UI
# -------------------------
if "username" not in st.session_state:
    st.session_state.username = ""

if st.session_state.username == "":
    st.session_state.username = st.text_input("ãƒ‹ãƒƒã‚¯ãƒãƒ¼ãƒ ã‚’å…¥åŠ›ã—ã¦Enterã‚’æŠ¼ã—ã¦ãã ã•ã„", key="username_input")
    if st.session_state.username:
        st.rerun()
else:
    st.write(f"ã“ã‚“ã«ã¡ã¯ã€{st.session_state.username}ã•ã‚“ï¼")

    if "messages" not in st.session_state:
        st.session_state.messages = []

    # è¡¨ç¤º
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
            if message["role"] == "assistant" and message.get("source_documents"):
                with st.expander("ğŸ” å›ç­”ã®æ ¹æ‹ ã¨ãªã£ãŸãƒ†ã‚­ã‚¹ãƒˆ"):
                    for doc in message["source_documents"]:
                        src = doc.metadata.get("source_video", "ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æœªç™»éŒ²")
                        url = doc.metadata.get("url", "#")
                        st.markdown(f"**å‚ç…§å…ƒ:** [{src}]({url})")
                        st.write(f"> {doc.page_content}")

    # å…¥åŠ›
    if query := st.chat_input("ğŸ’¬ å‡½é¤¨ã®è¡—æ­©ãã«åŸºã¥ã„ã¦è³ªå•ã—ã¦ã¿ã¦ãã ã•ã„"):
        st.session_state.messages.append({"role": "user", "content": query})
        with st.chat_message("user"):
            st.markdown(query)

        with st.chat_message("assistant"):
            with st.spinner("è€ƒãˆä¸­..."):
                # ä¼šè©±å±¥æ­´ã®æ•´å½¢ï¼ˆ(user, assistant) ãƒšã‚¢ï¼‰
                chat_history = []
                for msg in st.session_state.messages[:-1]:
                    if msg["role"] == "user":
                        chat_history.append((msg["content"], ""))
                    elif msg["role"] == "assistant" and chat_history:
                        last_q, _ = chat_history[-1]
                        chat_history[-1] = (last_q, msg["content"])

                # å®Ÿè¡Œï¼ˆEnsembleRetriever ã‚’åˆ©ç”¨ï¼‰
                result = qa_chain({"question": query, "chat_history": chat_history})
                response = result.get("answer", "")

                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹è¡¨ç¤º
                st.markdown(response)

                # å‚è€ƒæ–‡çŒ®è¡¨ç¤ºï¼ˆãƒ•ã‚£ãƒ«ã‚¿æ¸ˆã®ã‚½ãƒ¼ã‚¹ã‚’è¡¨ç¤ºï¼‰
                with st.expander("ğŸ” å›ç­”ã®æ ¹æ‹ ã¨ãªã£ãŸãƒ†ã‚­ã‚¹ãƒˆ"):
                    src_docs = result.get("source_documents", [])
                    # src_docs ã¯ ensemble ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆBM25ã¯ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚’ä¸ãˆã¦ã„ã‚‹ã®ã§ page_content ãŒãƒˆãƒ¼ã‚¯ãƒ³åˆ—ã®ã‚‚ã®ã‚‚æ··ã–ã‚‹ï¼‰
                    # è¡¨ç¤ºç”¨ã«ã¯ã€ã‚‚ã— metadata ã«å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚ã¦ã„ã‚Œã°ãã‚Œã‚’ä½¿ã†è¨­è¨ˆãŒæœ›ã¾ã—ã„ã€‚
                    for doc in src_docs:
                        # ã‚‚ã— BM25 å´ã® tokenized text ãŒæ¸¡ã£ã¦ããŸã‚‰ã€çŸ­ã™ãã‚‹å ´åˆã¯ metadata ã®ã‚ªãƒªã‚¸ãƒŠãƒ«ã‚’ä½¿ã†å·¥å¤«ã‚’ã—ã¾ã—ã‚‡ã†
                        text_to_show = doc.page_content
                        if len(text_to_show) < 50 and doc.metadata.get("url"):
                            # å¯èƒ½ã§ã‚ã‚Œã°ã€raw_data ã‹ã‚‰å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¢ã—ã¦è¡¨ç¤ºï¼ˆç°¡æ˜“å‡¦ç†ï¼‰
                            text_to_show = doc.metadata.get("orig_text", doc.page_content)
                        st.write(text_to_show)
                        st.markdown(f"**å‚ç…§å…ƒ:** {doc.metadata.get('source_video','ä¸æ˜ãªã‚½ãƒ¼ã‚¹')}  |  {doc.metadata.get('url','#')}")

                # ãƒ­ã‚°ä¿å­˜
                append_log_to_gsheet(worksheet, st.session_state.username, query, response)

                # ä¼šè©±å±¥æ­´ã«è¿½åŠ ï¼ˆassistantï¼‰
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": response,
                    "source_documents": result.get("source_documents", [])
                })
